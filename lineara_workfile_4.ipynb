{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ns329APPQlk1"
   },
   "source": [
    "# Lost in Translation: Computational Approach to Linear A Decryption with LSTM and Transformer Models\n",
    "### *Team: Steven Lu, Georgiy Sekretaryuk, Oluwafemi*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68cUlXO-Q1PE"
   },
   "source": [
    "## OUTLINE\n",
    "\n",
    "Part 1 Goals:\n",
    "- replicate NeuroDecipher LSTM model with Linear B\n",
    "- apply NeuroDecipher NLP approaches in a transformer model\n",
    "- test different pre-training techniques and parameters to see how it influences the result\n",
    "\n",
    "Part 2 Goals:\n",
    "\n",
    "...TBD after Nov 13\n",
    "- Work with Linear A here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A95gI6tYhP19"
   },
   "source": [
    "## IMPORTS\n",
    "\n",
    "Import the necessary libraries for the project and define any additional configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "T3eQrjVocsjt",
    "outputId": "5f007507-1e9a-4379-cbd2-aa95e3baca43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (4.33.3)\n",
      "Requirement already satisfied: filelock in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (1.23.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (2.30.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: torch in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transliterate in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (1.10.2)\n",
      "Requirement already satisfied: six>=1.1.0 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transliterate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# IMPORT THE LIBRARIES HERE\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install transliterate\n",
    "!pip install sentencepiece\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from transliterate import translit, get_available_language_codes\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2ovQjQW5V2Zn"
   },
   "outputs": [],
   "source": [
    "#setup for GDrive\n",
    "# #@title SELECT USER to mount the data drive according to its path in your drive\n",
    "# USER = 'Georgiy' #@param ['Georgiy', 'Steven', 'Oluwafemi']\n",
    "\n",
    "# #@title Mount GDrive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# #remove cache\n",
    "# !rm -rf \"/content/drive/MyDrive/NLP_266/__pycache__\"\n",
    "\n",
    "# #@title Set PATH to /data/ folder\n",
    "# PATHS = {}\n",
    "# PATHS['Georgiy'] = \"/content/drive/MyDrive/NLP_266\"\n",
    "# PATHS['Steven'] = \"/content/drive/Shareddrives/PathForSteven\"  # Replace with the actual path\n",
    "# PATHS['Oluwafemi'] = \"/content/drive/Shareddrives/PathForOluwafemi\"  # Replace with the actual path\n",
    "# PATH = PATHS[USER]\n",
    "\n",
    "# if PATH == \"\":\n",
    "#     raise ValueError(\"Enter your path to the shared data folder.\\nIt should start with 'content/drive/...' and end with '.../281 Final Project/data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nD6I0mNzQjqj"
   },
   "outputs": [],
   "source": [
    "# # Import Lin B from NeuroDecipher https://github.com/j-luo93/NeuroDecipher\n",
    "#only run this if the NeuroDecipher folder is empty\n",
    "# folder_path = 'NeuroDecipher'\n",
    "\n",
    "# if os.path.exists(folder_path):\n",
    "#    shutil.rmtree(folder_path)\n",
    "#    print(f\"The folder '{folder_path}' has been removed.\")\n",
    "# else:\n",
    "#    print(f\"The folder '{folder_path}' does not exist.\")\n",
    "\n",
    "# !git clone https://github.com/j-luo93/NeuroDecipher\n",
    "# !git submodule init && git submodule update\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !cd NeuroDecipher && pip install -r requirements.txt\n",
    "# !cd NeuroDecipher && pip install .\n",
    "# !cd NeuroDecipher/arglib && ls\n",
    "# !cd NeuroDecipher/editdistance && pip install .\n",
    "# !cd NeuroDecipher/arglib && pip install .\n",
    "# !cd NeuroDecipher/dev_misc && pip install -r requirements.txt\n",
    "# !cd NeuroDecipher/dev_misc && pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvdwnoAuQ7qi"
   },
   "source": [
    "## LOAD THE DATA\n",
    "\n",
    "Load the data from https://github.com/j-luo93/NeuroDecipher.\n",
    "\n",
    "Each .cog file is essentially a tsv file, where each column corresponds to the words in one language. Words in the same row are considered cognates. If for one word, there is no corresponding cognate in another language, _ is used to fill the cell. If multiple cognates are available for the same word, '|' is used to separate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pms31bkmRAHd",
    "outputId": "31fe6d17-33ce-4c67-95b3-f6956a0d691f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Linear B Cognates before modifications:\n",
      "     linear_b              greek\n",
      "0      êÄÄêÄÅêÄ™êÄ¶êÄ≤          Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ\n",
      "1       êÄÄêÄÅêÄ¥êÄµ     Œ±ŒµŒ∏ŒπœÉœÑŒøœÇ|ŒµŒ∏ŒπŒ∂œâ\n",
      "2       êÄÄêÄÖêÄîêÄÉ      Œ±Œ¥Œ±ŒºŒ±Œø|Œ±Œ¥Œ±ŒºŒ±œÇ\n",
      "3       êÄÄêÄÖêÄïêÄ∏  Œ±Œ¥Œ±ŒºŒµfŒµŒπœÇ|Œ±Œ¥Œ±ŒºŒµœÖœÇ\n",
      "4      êÄÄêÄÖêÄ®êÄ¥êÄç          Œ±Œ¥œÅŒ±œÉœÑŒπŒøœÇ\n",
      "..       ...                ...\n",
      "914     êÅÜêÄØêÄäêÄí          œÜœÖœÉŒπŒ±œÅœáŒøœÇ\n",
      "915       êÅÜêÄ≥              œÜœÖœÑŒµœÅ\n",
      "916     êÅÜêÄ≥êÄ™êÄä            œÜœÖœÑŒµœÅŒπŒ±\n",
      "917   êÅÜêÅàêÄÄêÄêêÄ©êÄÑ       œÜœÖŒªŒπŒ±œÇŒ±Œ≥œÅŒµœÖœÇ\n",
      "918       êÅáêÄú             œÜœÑŒµŒΩŒøŒπ\n",
      "\n",
      "[919 rows x 2 columns]\n",
      "Loaded Linear B Names before modifications:\n",
      "     linear_b              greek\n",
      "0      êÄÄêÄÅêÄ™êÄ¶êÄ≤          Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ\n",
      "1       êÄÄêÄÅêÄ¥êÄµ                  _\n",
      "2       êÄÄêÄÖêÄîêÄÉ      Œ±Œ¥Œ±ŒºŒ±Œø|Œ±Œ¥Œ±ŒºŒ±œÇ\n",
      "3       êÄÄêÄÖêÄïêÄ∏  Œ±Œ¥Œ±ŒºŒµfŒµŒπœÇ|Œ±Œ¥Œ±ŒºŒµœÖœÇ\n",
      "4      êÄÄêÄÖêÄ®êÄ¥êÄç          Œ±Œ¥œÅŒ±œÉœÑŒπŒøœÇ\n",
      "..       ...                ...\n",
      "914     êÅÜêÄØêÄäêÄí          œÜœÖœÉŒπŒ±œÅœáŒøœÇ\n",
      "915       êÅÜêÄ≥                  _\n",
      "916     êÅÜêÄ≥êÄ™êÄä                  _\n",
      "917   êÅÜêÅàêÄÄêÄêêÄ©êÄÑ       œÜœÖŒªŒπŒ±œÇŒ±Œ≥œÅŒµœÖœÇ\n",
      "918       êÅáêÄú                  _\n",
      "\n",
      "[919 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the data into a pandas DataFrame\n",
    "file_path = 'NeuroDecipher/data/linear_b-greek.cog'\n",
    "file_path_names = 'NeuroDecipher/data/linear_b-greek.names.cog'\n",
    "data_linearb = pd.read_csv(file_path, sep='\\t', header=0)\n",
    "data_linearb_names = pd.read_csv(file_path_names, sep='\\t', header=0)\n",
    "\n",
    "# Print data for testing\n",
    "print('Loaded Linear B Cognates before modifications:\\n', data_linearb)\n",
    "print('Loaded Linear B Names before modifications:\\n', data_linearb_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg7KzeerYJdV"
   },
   "source": [
    "## DATA MODIFICATION\n",
    "\n",
    "- Do we split the data into individual letters?\n",
    "\n",
    "- INstead of columns for cog 1 / cog 2, turn it into rows -> increases dataset size\n",
    "- turn empty rows into test/train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HdD7NquyYRzk",
    "outputId": "66abc283-72b0-4b91-c410-bb249ecf938f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0        1     2     3     4\n",
      "0  Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ     None  None  None  None\n",
      "1          _     None  None  None  None\n",
      "2     Œ±Œ¥Œ±ŒºŒ±Œø   Œ±Œ¥Œ±ŒºŒ±œÇ  None  None  None\n",
      "3  Œ±Œ¥Œ±ŒºŒµfŒµŒπœÇ  Œ±Œ¥Œ±ŒºŒµœÖœÇ  None  None  None\n",
      "4  Œ±Œ¥œÅŒ±œÉœÑŒπŒøœÇ     None  None  None  None\n"
     ]
    }
   ],
   "source": [
    "# @title Modify the Data\n",
    "\n",
    "# LINEAR B COGNATES\n",
    "\n",
    "# Renaming the original greek column to track the original\n",
    "data_linearb.rename(columns={'greek': 'greek_original'}, inplace=True)\n",
    "# Split the 'Greek' col into 2\n",
    "split_columns = data_linearb['greek_original'].str.split('|', expand=True)\n",
    "\n",
    "# Assigning split cols\n",
    "data_linearb['greek_cog_1'] = split_columns[0]\n",
    "data_linearb['greek_cog_2'] = split_columns[1].fillna('')\n",
    "data_linearb['greek_cog_3'] = split_columns[2].fillna('')\n",
    "data_linearb['greek_cog_4'] = split_columns[3].fillna('')\n",
    "data_linearb['greek_cog_5'] = split_columns[4].fillna('')\n",
    "\n",
    "# LINEAR B NAMES\n",
    "\n",
    "data_linearb_names.rename(columns={'greek': 'greek_original'}, inplace=True)\n",
    "# Split the 'Greek' col into 2\n",
    "split_columns = data_linearb_names['greek_original'].str.split('|', expand=True)\n",
    "print(split_columns.head()) #max: 5\n",
    "# Assigning split cols\n",
    "data_linearb_names['greek_cog_1'] = split_columns[0]\n",
    "data_linearb_names['greek_cog_2'] = split_columns[1].fillna('')\n",
    "data_linearb_names['greek_cog_3'] = split_columns[2].fillna('')\n",
    "data_linearb_names['greek_cog_4'] = split_columns[3].fillna('')\n",
    "data_linearb_names['greek_cog_5'] = split_columns[4].fillna('')\n",
    "# Replace all _ with blank space\n",
    "data_linearb_names.replace('_', '', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert(len(data_linearb)==len(data_linearb_names))\n",
    "data_linearb_split=[]\n",
    "data_linearb_names_split=[]\n",
    "for i in range(len(data_linearb)):\n",
    "    #fill linear B\n",
    "    temp=[data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_1\"].iloc[i]]\n",
    "    data_linearb_split.append(temp)\n",
    "    if data_linearb[\"greek_cog_2\"].iloc[i]!=\"\":\n",
    "        data_linearb_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_2\"].iloc[i]])\n",
    "    if data_linearb[\"greek_cog_3\"].iloc[i]!=\"\":\n",
    "        data_linearb_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_3\"].iloc[i]])\n",
    "    if data_linearb[\"greek_cog_4\"].iloc[i]!=\"\":\n",
    "        data_linearb_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_4\"].iloc[i]])\n",
    "    if data_linearb[\"greek_cog_5\"].iloc[i]!=\"\":\n",
    "        data_linearb_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_5\"].iloc[i]])\n",
    "    #fill linear B names\n",
    "    temp=[data_linearb_names[\"linear_b\"].iloc[i],data_linearb_names[\"greek_cog_1\"].iloc[i]]\n",
    "    data_linearb_names_split.append(temp)\n",
    "    if data_linearb_names[\"greek_cog_2\"].iloc[i]!=\"\":\n",
    "        data_linearb_names_split.append([data_linearb_names[\"linear_b\"].iloc[i],data_linearb_names[\"greek_cog_2\"].iloc[i]])\n",
    "    if data_linearb_names[\"greek_cog_3\"].iloc[i]!=\"\":\n",
    "        data_linearb_names_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_3\"].iloc[i]])\n",
    "    if data_linearb_names[\"greek_cog_4\"].iloc[i]!=\"\":\n",
    "        data_linearb_names_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_4\"].iloc[i]])\n",
    "    if data_linearb_names[\"greek_cog_5\"].iloc[i]!=\"\":\n",
    "        data_linearb_names_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_5\"].iloc[i]])\n",
    "data_linearb_split=pd.DataFrame(data_linearb_split,columns=[\"linear_b\",\"greek\"])\n",
    "data_linearb_names_split=pd.DataFrame(data_linearb_names_split,columns=[\"linear_b\",\"greek\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 919 lines\n",
      "  linear_b     greek_original greek_cog_1 greek_cog_2 greek_cog_3 greek_cog_4   \n",
      "0    êÄÄêÄÅêÄ™êÄ¶êÄ≤          Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ   Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ                                      \\\n",
      "1     êÄÄêÄÅêÄ¥êÄµ     Œ±ŒµŒ∏ŒπœÉœÑŒøœÇ|ŒµŒ∏ŒπŒ∂œâ    Œ±ŒµŒ∏ŒπœÉœÑŒøœÇ       ŒµŒ∏ŒπŒ∂œâ                           \n",
      "2     êÄÄêÄÖêÄîêÄÉ      Œ±Œ¥Œ±ŒºŒ±Œø|Œ±Œ¥Œ±ŒºŒ±œÇ      Œ±Œ¥Œ±ŒºŒ±Œø      Œ±Œ¥Œ±ŒºŒ±œÇ                           \n",
      "3     êÄÄêÄÖêÄïêÄ∏  Œ±Œ¥Œ±ŒºŒµfŒµŒπœÇ|Œ±Œ¥Œ±ŒºŒµœÖœÇ   Œ±Œ¥Œ±ŒºŒµfŒµŒπœÇ     Œ±Œ¥Œ±ŒºŒµœÖœÇ                           \n",
      "4    êÄÄêÄÖêÄ®êÄ¥êÄç          Œ±Œ¥œÅŒ±œÉœÑŒπŒøœÇ   Œ±Œ¥œÅŒ±œÉœÑŒπŒøœÇ                                       \n",
      "\n",
      "  greek_cog_5  \n",
      "0              \n",
      "1              \n",
      "2              \n",
      "3              \n",
      "4               \n",
      "\n",
      "Split: 1429 lines\n",
      "  linear_b      greek\n",
      "0    êÄÄêÄÅêÄ™êÄ¶êÄ≤  Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ\n",
      "1     êÄÄêÄÅêÄ¥êÄµ   Œ±ŒµŒ∏ŒπœÉœÑŒøœÇ\n",
      "2     êÄÄêÄÅêÄ¥êÄµ      ŒµŒ∏ŒπŒ∂œâ\n",
      "3     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±Œø\n",
      "4     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±œÇ \n",
      "\n",
      "Value: êÄ©êÄ∫ Split Count: [5] Original Count: 7\n",
      "Value: êÄµêÄ•êÄÜ Split Count: [5] Original Count: 6\n",
      "\n",
      "We have 917 matches.\n"
     ]
    }
   ],
   "source": [
    "# Counting Linear B original dataset and split dataset.\n",
    "\n",
    "print(\"Original:\",len(data_linearb),\"lines\")\n",
    "print(data_linearb.head(),'\\n')\n",
    "print(\"Split:\",len(data_linearb_split),\"lines\")\n",
    "print(data_linearb_split.head(),'\\n')\n",
    "\n",
    "##########################\n",
    "###### SANITY CHECK ######\n",
    "##########################\n",
    "\n",
    "data_linearb_split_count = data_linearb_split[\"linear_b\"].value_counts().reset_index()\n",
    "data_linearb_split_count.columns = [\"linear_b\", \"count\"]\n",
    "# print(data_linearb_split_count)\n",
    "\n",
    "# Count how many greek definitions each linear b value has in the original table, separeted by '|'\n",
    "data_linearb_count = data_linearb[\"greek_original\"].apply(lambda x: 0 if pd.isna(x) else (1 if '|' not in x else x.count('|') + 1))\n",
    "data_linearb_count = pd.DataFrame({\"linear_b\": data_linearb[\"linear_b\"], \"count\": data_linearb_count})\n",
    "# print(data_linearb_count)\n",
    "\n",
    "# The values should match. If they don't, print out the rows that don't match. Otherwise, print out the number of matches.\n",
    "match_count = 0\n",
    "for index, row in data_linearb_count.iterrows():\n",
    "    split_count = data_linearb_split_count[data_linearb_split_count['linear_b'] == row['linear_b']]['count'].values\n",
    "    original_count = row['count']\n",
    "    if split_count != original_count:\n",
    "        print(\"Value:\", row['linear_b'], \"Split Count:\", split_count, \"Original Count:\", original_count)\n",
    "    else:\n",
    "        match_count += 1\n",
    "print(f\"\\nWe have {match_count} matches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 919 lines\n",
      "  linear_b     greek_original greek_cog_1 greek_cog_2 greek_cog_3 greek_cog_4   \n",
      "0    êÄÄêÄÅêÄ™êÄ¶êÄ≤          Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ   Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ                                      \\\n",
      "1     êÄÄêÄÅêÄ¥êÄµ                                                                      \n",
      "2     êÄÄêÄÖêÄîêÄÉ      Œ±Œ¥Œ±ŒºŒ±Œø|Œ±Œ¥Œ±ŒºŒ±œÇ      Œ±Œ¥Œ±ŒºŒ±Œø      Œ±Œ¥Œ±ŒºŒ±œÇ                           \n",
      "3     êÄÄêÄÖêÄïêÄ∏  Œ±Œ¥Œ±ŒºŒµfŒµŒπœÇ|Œ±Œ¥Œ±ŒºŒµœÖœÇ   Œ±Œ¥Œ±ŒºŒµfŒµŒπœÇ     Œ±Œ¥Œ±ŒºŒµœÖœÇ                           \n",
      "4    êÄÄêÄÖêÄ®êÄ¥êÄç          Œ±Œ¥œÅŒ±œÉœÑŒπŒøœÇ   Œ±Œ¥œÅŒ±œÉœÑŒπŒøœÇ                                       \n",
      "\n",
      "  greek_cog_5  \n",
      "0              \n",
      "1              \n",
      "2              \n",
      "3              \n",
      "4              \n",
      "Split: 1069 lines\n",
      "  linear_b      greek\n",
      "0    êÄÄêÄÅêÄ™êÄ¶êÄ≤  Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ\n",
      "1     êÄÄêÄÅêÄ¥êÄµ           \n",
      "2     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±Œø\n",
      "3     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±œÇ\n",
      "4     êÄÄêÄÖêÄïêÄ∏  Œ±Œ¥Œ±ŒºŒµfŒµŒπœÇ\n",
      "\n",
      "We have 919 matches.\n"
     ]
    }
   ],
   "source": [
    "#Counting Linear B Names and split names.\n",
    "\n",
    "# print('\\n ------ LINEAR B NAMES -----\\n')\n",
    "print(\"Original:\",len(data_linearb_names),\"lines\")\n",
    "print(data_linearb_names.head())\n",
    "print(\"Split:\",len(data_linearb_names_split),\"lines\")\n",
    "print(data_linearb_names_split.head())\n",
    "\n",
    "##########################\n",
    "###### SANITY CHECK ######\n",
    "##########################\n",
    "\n",
    "# Count how many times each linear b value appears int he split.\n",
    "data_linearb_names_split_count = data_linearb_names_split[\"linear_b\"].value_counts().reset_index()\n",
    "data_linearb_names_split_count.columns = [\"linear_b\", \"count\"]\n",
    "# print(data_linearb_names_split_count)\n",
    "\n",
    "# Count how many greek definitions each linear b value has in the original table, separeted by '|'\n",
    "data_linearb_names_count = data_linearb_names[\"greek_original\"].apply(lambda x: 0 if pd.isna(x) else (1 if '|' not in x else x.count('|') + 1))\n",
    "data_linearb_names_count = pd.DataFrame({\"linear_b\": data_linearb_names[\"linear_b\"], \"count\": data_linearb_names_count})\n",
    "# print(data_linearb_names_count)\n",
    "\n",
    "# The values should match. If they don't, print out the rows that don't match. Otherwise, print out the number of matches.\n",
    "match_count = 0\n",
    "for index, row in data_linearb_names_count.iterrows():\n",
    "    split_count = data_linearb_names_split_count[data_linearb_names_split_count['linear_b'] == row['linear_b']]['count'].values\n",
    "    original_count = row['count']\n",
    "    if split_count != original_count:\n",
    "        print(\"Value:\", row['linear_b'], \"Split Count:\", split_count, \"Original Count:\", original_count)\n",
    "    else:\n",
    "        match_count += 1\n",
    "print(f\"\\nWe have {match_count} matches.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking unique values in each column:\n",
      "\n",
      "data_linearb:\n",
      "\n",
      "linear_b Unique Values: 919\n",
      "greek_original Unique Values: 918\n",
      "greek_cog_1 Unique Values: 918\n",
      "greek_cog_2 Unique Values: 388\n",
      "greek_cog_3 Unique Values: 87\n",
      "greek_cog_4 Unique Values: 28\n",
      "greek_cog_5 Unique Values: 7\n",
      "\n",
      "\n",
      "data_linearb_names:\n",
      "\n",
      "linear_b Unique Values: 919\n",
      "greek_original Unique Values: 456\n",
      "greek_cog_1 Unique Values: 456\n",
      "greek_cog_2 Unique Values: 131\n",
      "greek_cog_3 Unique Values: 16\n",
      "greek_cog_4 Unique Values: 5\n",
      "greek_cog_5 Unique Values: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking unique values in each column:\\n\")\n",
    "\n",
    "print(\"data_linearb:\\n\")\n",
    "for col in data_linearb.columns:\n",
    "    if not isinstance(data_linearb[col].iloc[0], list):\n",
    "        print(f\"{col} Unique Values:\", data_linearb[col].nunique())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"data_linearb_names:\\n\")\n",
    "for col in data_linearb_names.columns:\n",
    "    print(f\"{col} Unique Values:\", data_linearb_names[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO by Sunday:\\n\\n[DONE]- Cycle through greek translations of linear B with more than 2 translations (separeted by \\'|\\' separator), create n columns where n is max count of translations in the largest row\\n\\n[DONE]- INstead of columns for cog 1 - cog n, turn it into rows -> increases dataset size \\n[DONE]- TWEAK - see above \\n[DONE]- Determine which are names and not names\\n#done- see below\\n    - For each language, create an object. In the \\n      object store data for the alphabet and the universal syllabic translation.\\n\\n\\n#OLD:\\n# - Create a mapping dict for universal character embeddings for linear b and for greek\\n#     - Create a unersal syllable matrix\\n# - Map linear b to universal syllables (matrix)\\n# - Map greek to universal syllables (matrix)\\n\\n#NEW:\\n[DONE]- Transliterate Linear B - done\\n[DONE]- Transliterate Modern Greek - done\\n\\n- Cycle through each greek word. Find word with highest \"syllabic matching\" to linear B and use that word for the model.\\nêÄÄêÄáêÄ™êÄäêÄ†\\tŒ±ŒΩŒ¥œÅŒπŒ±ŒΩœÑŒµŒπ|Œ±ŒΩŒ¥œÅŒπŒ±œÜŒπ|Œ±ŒΩŒ¥œÅŒπŒøœÇ|Œ±ŒΩŒµœÅ\\n\\n- turn empty rows into train/test\\n#Steven - done: see above\\n- Create a train/test split of 20/80 (50/50 distribution of names/not name cognates?)\\n\\n\\n- Identify separators for transliterated Linear B\\n  - Separators between characters; separators between words\\n\\n- UNKNOWN: separators for transliterated Greek\\n  - Q: How do we set up the model to predict this...\\n\\n- Model BART, T5\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO by Sunday:\n",
    "\n",
    "[DONE]- Cycle through greek translations of linear B with more than 2 translations (separeted by '|' separator), create n columns where n is max count of translations in the largest row\n",
    "\n",
    "[DONE]- INstead of columns for cog 1 - cog n, turn it into rows -> increases dataset size \n",
    "[DONE]- TWEAK - see above \n",
    "[DONE]- Determine which are names and not names\n",
    "#done- see below\n",
    "    - For each language, create an object. In the \n",
    "      object store data for the alphabet and the universal syllabic translation.\n",
    "\n",
    "\n",
    "#OLD:\n",
    "# - Create a mapping dict for universal character embeddings for linear b and for greek\n",
    "#     - Create a unersal syllable matrix\n",
    "# - Map linear b to universal syllables (matrix)\n",
    "# - Map greek to universal syllables (matrix)\n",
    "\n",
    "#NEW:\n",
    "[DONE]- Transliterate Linear B - done\n",
    "[DONE]- Transliterate Modern Greek - done\n",
    "\n",
    "- Cycle through each greek word. Find word with highest \"syllabic matching\" to linear B and use that word for the model.\n",
    "êÄÄêÄáêÄ™êÄäêÄ†\tŒ±ŒΩŒ¥œÅŒπŒ±ŒΩœÑŒµŒπ|Œ±ŒΩŒ¥œÅŒπŒ±œÜŒπ|Œ±ŒΩŒ¥œÅŒπŒøœÇ|Œ±ŒΩŒµœÅ\n",
    "\n",
    "- turn empty rows into train/test\n",
    "#Steven - done: see above\n",
    "- Create a train/test split of 20/80 (50/50 distribution of names/not name cognates?)\n",
    "\n",
    "\n",
    "- Identify separators for transliterated Linear B\n",
    "  - Separators between characters; separators between words\n",
    "\n",
    "- UNKNOWN: separators for transliterated Greek\n",
    "  - Q: How do we set up the model to predict this...\n",
    "\n",
    "- Model BART, T5\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeriqota\n"
     ]
    }
   ],
   "source": [
    "# linear b syllabic mapping from NeuroDecipher MIT paper\n",
    "\n",
    "linb2syl = {\n",
    "    u'êÄÄ': 'a', u'êÄÅ': 'e', u'êÄÇ': 'i', u'êÄÉ': 'o', u'êÄÑ': 'u', u'êÄÖ': 'da', u'êÄÜ': 'de', \n",
    "    u'êÄá': 'di', u'êÄà': 'do', u'êÄâ': 'du', u'êÄä': 'ja', u'êÄã': 'je', u'êÄç': 'jo', \n",
    "    u'êÄé': 'ju', u'êÄè': 'ka', u'êÄê': 'ke', u'êÄë': 'ki', u'êÄí': 'ko', u'êÄì': 'ku', \n",
    "    u'êÄî': 'ma', u'êÄï': 'me', u'êÄñ': 'mi', u'êÄó': 'mo', u'êÄò': 'mu', u'êÄô': 'na', \n",
    "    u'êÄö': 'ne', u'êÄõ': 'ni', u'êÄú': 'no', u'êÄù': 'nu', u'êÄû': 'pa', u'êÄü': 'pe', \n",
    "    u'êÄ†': 'pi', u'êÄ°': 'po', u'êÄ¢': 'pu', u'êÄ£': 'qa', u'êÄ§': 'qe', u'êÄ•': 'qi', \n",
    "    u'êÄ¶': 'qo', u'êÄ®': 'ra', u'êÄ©': 're', u'êÄ™': 'ri', u'êÄ´': 'ro', u'êÄ¨': 'ru',\n",
    "    u'êÄ≠': 'sa', u'êÄÆ': 'se', u'êÄØ': 'si', u'êÄ∞': 'so', u'êÄ±': 'su', u'êÄ≤': 'ta', \n",
    "    u'êÄ≥': 'te', u'êÄ¥': 'ti', u'êÄµ': 'to', u'êÄ∂': 'tu', u'êÄ∑': 'wa', u'êÄ∏': 'we', \n",
    "    u'êÄπ': 'wi', u'êÄ∫': 'wo', u'êÄº': 'za', u'êÄΩ': 'ze', u'êÄø': 'zo', u'êÅÄ': 'a2', \n",
    "    u'êÅÅ': 'a3', u'êÅÇ': 'au', u'êÅÉ': 'dwe', u'êÅÑ': 'dwo', u'êÅÖ': 'nwa', u'êÅÜ': 'pu2', \n",
    "    u'êÅá': 'pte', u'êÅà': 'ra2', u'êÅâ': 'ra3', u'êÅä': 'ro2', u'êÅã': 'ta2', u'êÅå': 'twe', u'êÅç': 'two'\n",
    "}\n",
    "\n",
    "def transliterate_linb(word,dic):\n",
    "    \"\"\"Transliterate Linear B characters into syllables.\"\"\"\n",
    "    \n",
    "    res=\"\" #result\n",
    "\n",
    "    # Cycle through each character in the Linear B word\n",
    "    for ch in word:\n",
    "\n",
    "        # translation is the value for that character's key in the mapping dictionary\n",
    "        trans=dic[ch]\n",
    "        res+=trans\n",
    "\n",
    "    return res\n",
    "\n",
    "#test\n",
    "print(transliterate_linb(\"êÄÄêÄÅêÄ™êÄ¶êÄ≤\",linb2syl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current names dataset: 1069\n",
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate\n",
      "0    êÄÄêÄÅêÄ™êÄ¶êÄ≤  Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ     1           aelipotas               aeriqota\n",
      "1     êÄÄêÄÅêÄ¥êÄµ                0                                     aetito\n",
      "2     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±Œø     1              adamao                 adamao\n",
      "3     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±œÇ     1              adamas                 adamao\n",
      "4     êÄÄêÄÖêÄïêÄ∏  Œ±Œ¥Œ±ŒºŒµfŒµŒπœÇ     1           adamefeis                adamewe\n",
      "\n",
      "Current non-names dataset: 1429\n",
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate\n",
      "0    êÄÄêÄÅêÄ™êÄ¶êÄ≤  Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ     1           aelipotas               aeriqota\n",
      "1     êÄÄêÄÅêÄ¥êÄµ   Œ±ŒµŒ∏ŒπœÉœÑŒøœÇ     0           aethistos                 aetito\n",
      "2     êÄÄêÄÅêÄ¥êÄµ      ŒµŒ∏ŒπŒ∂œâ     0              ethizo                 aetito\n",
      "3     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±Œø     1              adamao                 adamao\n",
      "4     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±œÇ     1              adamas                 adamao\n"
     ]
    }
   ],
   "source": [
    "#put binary inside the split name data of whether the linear b value is a name or not. \n",
    "\n",
    "def name(row):\n",
    "    if row[\"greek\"]==\"\":\n",
    "        return 0\n",
    "    return 1\n",
    "data_linearb_names_split[\"Name\"]=data_linearb_names_split.apply(name,axis=1)\n",
    "print(\"Current names dataset:\", len(data_linearb_names_split))\n",
    "print(data_linearb_names_split.head())\n",
    "\n",
    "#apply to the original, non-name dataset as well\n",
    "name_binary=[]\n",
    "for i in range(len(data_linearb_split)):\n",
    "    cur_linearb=data_linearb_split[\"linear_b\"].iloc[i]\n",
    "    corresponding_name=data_linearb_names_split[data_linearb_names_split[\"linear_b\"]==cur_linearb][\"greek\"].iloc[0]\n",
    "    if corresponding_name==\"\":\n",
    "        name_binary.append(0)\n",
    "    else: \n",
    "        name_binary.append(1)\n",
    "print(\"\\nCurrent non-names dataset:\", len(data_linearb_split))\n",
    "data_linearb_split[\"Name\"]=name_binary\n",
    "print(data_linearb_split.head())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRANSLITERATION OF GREEK\n",
    "\n",
    "!pip install transliterate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transliterate Linear B Names \n",
    "\n",
    "# Create clean lists to store data\n",
    "greek_transliterate=[]\n",
    "greek_transliterate_names=[]\n",
    "from transliterate import translit, get_available_language_codes\n",
    "\n",
    "# Cycle through the dataset with the names\n",
    "for i in range(len(data_linearb_names_split)):\n",
    "    \n",
    "    #if blank, leave blank\n",
    "    if data_linearb_names_split[\"greek\"].iloc[i]==\"\":greek_transliterate_names.append(\"\")\n",
    "    \n",
    "    #if not blank, transliterate\n",
    "    else:greek_transliterate_names.append(translit(data_linearb_names_split[\"greek\"].iloc[i], reversed=True))\n",
    "\n",
    "# Cycle through the dataset with the non-names\n",
    "for i in range(len(data_linearb_split)):\n",
    "    \n",
    "    #if blank, leave blank\n",
    "    if data_linearb_split[\"greek\"].iloc[i]==\"\":greek_transliterate.append(\"\")\n",
    "    \n",
    "    #if not blank, transliterate\n",
    "    else:greek_transliterate.append(translit(data_linearb_split[\"greek\"].iloc[i], reversed=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transliterate Greek names\n",
    "\n",
    "# Create clean lists to store data\n",
    "linearb_transliterate=[]\n",
    "linearb_transliterate_names=[]\n",
    "\n",
    "# Cycle through the dataset with the names\n",
    "for i in range(len(data_linearb_names_split)):\n",
    "    \n",
    "    #if blank, leave blank\n",
    "    if data_linearb_names_split[\"linear_b\"].iloc[i]==\"\":linearb_transliterate_names.append(\"\")\n",
    "\n",
    "    #if not blank, transliterate\n",
    "    else:linearb_transliterate_names.append(transliterate_linb(data_linearb_names_split[\"linear_b\"].iloc[i], linb2syl))\n",
    "\n",
    "# Cycle through the dataset with the non-names\n",
    "for i in range(len(data_linearb_split)):\n",
    "    \n",
    "    #if blank, leave blank\n",
    "    if data_linearb_split[\"linear_b\"].iloc[i]==\"\":linearb_transliterate.append(\"\")\n",
    "    \n",
    "    #if not blank, transliterate\n",
    "    else:linearb_transliterate.append(transliterate_linb(data_linearb_split[\"linear_b\"].iloc[i], linb2syl))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate\n",
      "0    êÄÄêÄÅêÄ™êÄ¶êÄ≤  Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ     1           aelipotas               aeriqota\n",
      "1     êÄÄêÄÅêÄ¥êÄµ   Œ±ŒµŒ∏ŒπœÉœÑŒøœÇ     0           aethistos                 aetito\n",
      "2     êÄÄêÄÅêÄ¥êÄµ      ŒµŒ∏ŒπŒ∂œâ     0              ethizo                 aetito\n",
      "3     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±Œø     1              adamao                 adamao\n",
      "4     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±œÇ     1              adamas                 adamao\n",
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate\n",
      "0    êÄÄêÄÅêÄ™êÄ¶êÄ≤  Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ     1           aelipotas               aeriqota\n",
      "1     êÄÄêÄÅêÄ¥êÄµ                0                                     aetito\n",
      "2     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±Œø     1              adamao                 adamao\n",
      "3     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±œÇ     1              adamas                 adamao\n",
      "4     êÄÄêÄÖêÄïêÄ∏  Œ±Œ¥Œ±ŒºŒµfŒµŒπœÇ     1           adamefeis                adamewe\n"
     ]
    }
   ],
   "source": [
    "assert(len(greek_transliterate)==len(data_linearb_split))\n",
    "assert(len(greek_transliterate_names)==len(data_linearb_names_split))\n",
    "data_linearb_split[\"greek_transliterate\"]=greek_transliterate\n",
    "data_linearb_names_split[\"greek_transliterate\"]=greek_transliterate_names\n",
    "data_linearb_split[\"linear_b_transliterate\"]=linearb_transliterate\n",
    "data_linearb_names_split[\"linear_b_transliterate\"]=linearb_transliterate_names\n",
    "print(data_linearb_split.head())\n",
    "print(data_linearb_names_split.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn1-3Fg_Xyws"
   },
   "source": [
    "## EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "Analyze the dataset features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "DlKUCwmpX5zR",
    "outputId": "b2a34baa-18a4-4f21-dd4f-c838c4de7690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- DESCRIBING THE COGNATE DATA: -----\n",
      "\n",
      "              Name\n",
      "count  1429.000000\n",
      "mean      0.433170\n",
      "std       0.495687\n",
      "min       0.000000\n",
      "25%       0.000000\n",
      "50%       0.000000\n",
      "75%       1.000000\n",
      "max       1.000000\n",
      "\n",
      "----- INFO: -----\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1429 entries, 0 to 1428\n",
      "Data columns (total 5 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   linear_b                1429 non-null   object\n",
      " 1   greek                   1429 non-null   object\n",
      " 2   Name                    1429 non-null   int64 \n",
      " 3   greek_transliterate     1429 non-null   object\n",
      " 4   linear_b_transliterate  1429 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 55.9+ KB\n",
      "None\n",
      "\n",
      "----- CHECKING FOR MISSING VALUES: -----\n",
      "\n",
      "linear_b                  0\n",
      "greek                     0\n",
      "Name                      0\n",
      "greek_transliterate       0\n",
      "linear_b_transliterate    0\n",
      "dtype: int64\n",
      "\n",
      "----- CHECKING UNIQUE VALUES: -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics and exploration\n",
    "\n",
    "print('\\n----- DESCRIBING THE NON-NAME DATA: -----\\n')\n",
    "print(data_linearb_split.describe())\n",
    "\n",
    "print('\\n----- INFO: -----\\n')\n",
    "print(data_linearb_split.info())\n",
    "\n",
    "# Check for missing values\n",
    "print('\\n----- CHECKING FOR MISSING VALUES: -----\\n')\n",
    "print(data_linearb_split.isnull().sum())\n",
    "\n",
    "# Explore unique values and frequency distribution\n",
    "print('\\n----- CHECKING UNIQUE VALUES: -----\\n')\n",
    "# print(data_linearb_split['linear_b'].value_counts())\n",
    "# print(data_linearb['greek_original'].value_counts())\n",
    "# print(data_linearb['greek_cog_1'].value_counts())\n",
    "# print(data_linearb['greek_cog_2'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "EOtKu8kMS03N",
    "outputId": "e35b3c79-63f0-48be-ad06-50ff082cdbef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- DESCRIBING THE NAMES DATA: -----\n",
      "\n",
      "              Name\n",
      "count  1069.000000\n",
      "mean      0.565949\n",
      "std       0.495864\n",
      "min       0.000000\n",
      "25%       0.000000\n",
      "50%       1.000000\n",
      "75%       1.000000\n",
      "max       1.000000\n",
      "\n",
      "----- INFO: -----\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1069 entries, 0 to 1068\n",
      "Data columns (total 5 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   linear_b                1069 non-null   object\n",
      " 1   greek                   1069 non-null   object\n",
      " 2   Name                    1069 non-null   int64 \n",
      " 3   greek_transliterate     1069 non-null   object\n",
      " 4   linear_b_transliterate  1069 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 41.9+ KB\n",
      "None\n",
      "\n",
      "----- CHECKING FOR MISSING VALUES: -----\n",
      "\n",
      "linear_b                  0\n",
      "greek                     0\n",
      "Name                      0\n",
      "greek_transliterate       0\n",
      "linear_b_transliterate    0\n",
      "dtype: int64\n",
      "\n",
      "----- CHECKING UNIQUE VALUES: -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n----- DESCRIBING THE NAMES DATA: -----\\n')\n",
    "print(data_linearb_names_split.describe())\n",
    "\n",
    "print('\\n----- INFO: -----\\n')\n",
    "print(data_linearb_names_split.info())\n",
    "\n",
    "# Check for missing values\n",
    "print('\\n----- CHECKING FOR MISSING VALUES: -----\\n')\n",
    "print(data_linearb_names_split.isnull().sum())\n",
    "\n",
    "# Explore unique values and frequency distribution\n",
    "print('\\n----- CHECKING UNIQUE VALUES: -----\\n')\n",
    "# print(data_linearb_names_split['linear_b'].value_counts())\n",
    "# print(data_linearb_names_split['greek_original'].value_counts())\n",
    "# print(data_linearb_names_split['greek_cog_1'].value_counts())\n",
    "# print(data_linearb_names_split['greek_cog_2'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFOJEmRTS03O"
   },
   "source": [
    "## SPLITTING & TOKENIZATION\n",
    "\n",
    "- Breakdown the words into characters\n",
    "- ???\n",
    "- Split the data into test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "6UvDXgERS03M",
    "outputId": "532648f0-a511-442d-8eb7-84cd3f65a77d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linearb_names_train: 605\n",
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate\n",
      "0    êÄÄêÄÅêÄ™êÄ¶êÄ≤  Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ     1           aelipotas               aeriqota\n",
      "2     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±Œø     1              adamao                 adamao\n",
      "linearb_names_test: 464\n",
      "  linear_b greek  Name greek_transliterate linear_b_transliterate\n",
      "1     êÄÄêÄÅêÄ¥êÄµ           0                                     aetito\n",
      "7      êÄÄêÄÜêÄ≥           0                                      adete\n"
     ]
    }
   ],
   "source": [
    "#Creating the train/test split\n",
    "\n",
    "#only need to split names into train and test for now,\n",
    "#since the names has several hundred blanks while there are no blanks in the ovr data\n",
    "\n",
    "data_linearb_names_train=data_linearb_names_split[data_linearb_names_split[\"greek\"]!=\"\"]\n",
    "data_linearb_names_test=data_linearb_names_split[data_linearb_names_split[\"greek\"]==\"\"]\n",
    "print(\"linearb_names_train:\", len(data_linearb_names_train))\n",
    "print(data_linearb_names_train.head(2))\n",
    "print(\"linearb_names_test:\", len(data_linearb_names_test))\n",
    "print(data_linearb_names_test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "ChnA4BoES03O"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/41/hphs227s057g8xhj28fpxshr0000gn/T/ipykernel_46539/3918948329.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_linearb_names_train['linear_b_tokens'] = data_linearb_names_train['linear_b'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
      "/var/folders/41/hphs227s057g8xhj28fpxshr0000gn/T/ipykernel_46539/3918948329.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_linearb_names_train['greek_tokens'] = data_linearb_names_train['greek'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
      "/var/folders/41/hphs227s057g8xhj28fpxshr0000gn/T/ipykernel_46539/3918948329.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_linearb_names_test['linear_b_tokens'] = data_linearb_names_test['linear_b'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n"
     ]
    }
   ],
   "source": [
    "# @title: Splitting & tokenizing the data\n",
    "\n",
    "\n",
    "# IS THIS CORRECT???\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "data_linearb_names_train['linear_b_tokens'] = data_linearb_names_train['linear_b'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "data_linearb_names_train['greek_tokens'] = data_linearb_names_train['greek'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "data_linearb_names_test['linear_b_tokens'] = data_linearb_names_test['linear_b'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# these 2 columns don't exist. Why was this added?\n",
    "# data_linearb['greek_cog_1_tokens'] = data_linearb['greek_cog_1'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "# data_linearb['greek_cog_2_tokens'] = data_linearb['greek_cog_2'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True) if x else [])\n",
    "\n",
    "# NEED TO TOKENIZE NAMES AND OTHER DATASETS THAT ARE LOADED HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linearb_names_train: 605\n",
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate   \n",
      "0    êÄÄêÄÅêÄ™êÄ¶êÄ≤  Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ     1           aelipotas               aeriqota  \\\n",
      "2     êÄÄêÄÖêÄîêÄÉ     Œ±Œ¥Œ±ŒºŒ±Œø     1              adamao                 adamao   \n",
      "\n",
      "   linear_b_tokens                                       greek_tokens  \n",
      "0  [101, 100, 102]  [101, 1155, 29723, 29727, 18199, 29731, 29730,...  \n",
      "2  [101, 100, 102]  [101, 1155, 29722, 14608, 29728, 14608, 29730,...  \n",
      "\n",
      "linearb_names_test: 464\n",
      "  linear_b greek  Name greek_transliterate linear_b_transliterate   \n",
      "1     êÄÄêÄÅêÄ¥êÄµ           0                                     aetito  \\\n",
      "7      êÄÄêÄÜêÄ≥           0                                      adete   \n",
      "\n",
      "   linear_b_tokens  \n",
      "1  [101, 100, 102]  \n",
      "7  [101, 100, 102]  \n"
     ]
    }
   ],
   "source": [
    "print(\"linearb_names_train:\", len(data_linearb_names_train))\n",
    "print(data_linearb_names_train.head(2))\n",
    "print(\"\\nlinearb_names_test:\", len(data_linearb_names_test))\n",
    "print(data_linearb_names_test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hcpc27jJX7JU"
   },
   "source": [
    "## MODEL ARCHITECTURE\n",
    "\n",
    "- Identify baseline model\n",
    "- Test other Seq2seq models\n",
    "  - Transformer model - our own?\n",
    "  - Or can we modify BERT/another model and train it too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THE BART MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "# article_A = \"Input A\"\n",
    "# article_B = \"Expected Output B\"\n",
    "\n",
    "# model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "# tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# # translate Linear B to Greek\n",
    "# tokenizer.src_lang = \"ar_AR\"\n",
    "# encoded_ar = tokenizer(article_ar, return_tensors=\"pt\")\n",
    "# generated_tokens = model.generate(**encoded_ar, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n",
    "# tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/41/hphs227s057g8xhj28fpxshr0000gn/T/ipykernel_46539/2309461964.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['input_text'] = 'translate Linear B to Greek: ' + train_df['linear_b_transliterate']\n",
      "/var/folders/41/hphs227s057g8xhj28fpxshr0000gn/T/ipykernel_46539/2309461964.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['target_text'] = train_df['greek_transliterate']\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#Y105sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m source, target\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#Y105sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Load tokenizer and model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#Y105sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mt5-small\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#Y105sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m model \u001b[39m=\u001b[39m T5ForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mt5-small\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#Y105sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Create dataset and dataloader\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1124\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1124\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/site-packages/transformers/utils/import_utils.py:1112\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1110\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[1;32m   1111\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece\n",
    "\n",
    "train_df = data_linearb_names_train\n",
    "test_df = data_linearb_names_test\n",
    "\n",
    "# Prepare the data\n",
    "train_df['input_text'] = 'translate Linear B to Greek: ' + train_df['linear_b_transliterate']\n",
    "train_df['target_text'] = train_df['greek_transliterate']\n",
    "\n",
    "# Define a custom dataset\n",
    "class LinearBDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_texts = data['input_text']\n",
    "        self.target_texts = data['target_text']\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.tokenizer(self.input_texts[idx], padding='max_length', max_length=self.max_length, truncation=True, return_tensors='pt')\n",
    "        target = self.tokenizer(self.target_texts[idx], padding='max_length', max_length=self.max_length, truncation=True, return_tensors='pt')\n",
    "        return source, target\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "train_dataset = LinearBDataset(tokenizer, train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Training loop (simplified)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[0]['input_ids'].to(device)\n",
    "        attention_mask = batch[0]['attention_mask'].to(device)\n",
    "        labels = batch[1]['input_ids'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearb_names_test['input_text'] = 'translate Linear B to Greek: ' + linearb_names_test['linear_b_transliterate']\n",
    "test_dataset = LinearBDataset(tokenizer, linearb_names_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Function to generate predictions\n",
    "def generate_predictions(model, tokenizer, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Prepare batch data\n",
    "            input_ids = batch[0]['input_ids'].to(device)\n",
    "            attention_mask = batch[0]['attention_mask'].to(device)\n",
    "\n",
    "            # Generate prediction\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            predictions.append(predicted_text)\n",
    "    return predictions\n",
    "\n",
    "# Generate predictions\n",
    "predicted_transliterations = generate_predictions(model, tokenizer, test_loader)\n",
    "\n",
    "# Print results\n",
    "for i, row in linearb_names_test.iterrows():\n",
    "    print(f\"Linear B Transliterate: {row['linear_b_transliterate']}\")\n",
    "    print(f\"Predicted Greek Transliterate: {predicted_transliterations[i]}\")\n",
    "    print(f\"Actual Greek Transliterate: {row['greek_transliterate'] if 'greek_transliterate' in row else 'N/A'}\")\n",
    "    print(\"--------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## T5 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "t5_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = data_linearb_names_split['linear_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_input_texts = [\"translate LinearB to Greek: \" + str(entry) for entry in top_10_articles]\n",
    "t5_inputs = t5_tokenizer(t5_input_texts, return_tensors='tf', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                   num_beams=3,\n",
    "                                   no_repeat_ngram_size=3,\n",
    "                                   min_length=10,\n",
    "                                   max_length=40)\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BART MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any existing file/directory with the same name\n",
    "!rm -rf bart.large.tar.gz\n",
    "\n",
    "# Download the BART model\n",
    "!wget https://dl.fbaipublicfiles.com/fairseq/models/bart.large.tar.gz\n",
    "\n",
    "# Extract the tar file\n",
    "!tar -xzvf bart.large.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = './bart.large.tar.gz'\n",
    "bart = BARTModel.from_pretrained(model_directory, checkpoint_file='model.pt')\n",
    "bart.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = bart.encode('Œ±ŒµŒªŒπœÄŒøœÑŒ±œÇ')\n",
    "print(\"Encoded tokens:\", tokens.tolist())\n",
    "decoded_text = bart.decode(tokens)\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_features = bart.extract_features(tokens)\n",
    "\n",
    "assert last_layer_features.size() == torch.Size([1, len(tokens), bart.model.encoder.embed_tokens.embedding_dim])\n",
    "\n",
    "all_layers = bart.extract_features(tokens, return_all_hiddens=True)\n",
    "\n",
    "assert len(all_layers) == bart.model.encoder.layers.__len__() + 1  # +1 for the embedding layer\n",
    "assert torch.all(all_layers[-1] == last_layer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jXBtljgS03P"
   },
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUxpeJLjX89Y"
   },
   "outputs": [],
   "source": [
    "# Loading BERT\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "bert_model = BertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYBwnmy1S03P"
   },
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03H5YWQ3S03Q"
   },
   "outputs": [],
   "source": [
    "# Building the COgnate model (sample skeleton)\n",
    "\n",
    "class CognatePredictionModel(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(CognatePredictionModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "\n",
    "        # BERT outputs a 768-d vector\n",
    "        bert_output_size = 768\n",
    "\n",
    "        # Additional fully connected layers\n",
    "        self.fc1 = nn.Linear(bert_output_size * 2, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        # Output layer for binary classification\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, linear_b_tokens, greek_tokens):\n",
    "        # Pass input through BERT, take pooled output\n",
    "        outputs_linear_b = self.bert(linear_b_tokens)[1]\n",
    "        outputs_greek = self.bert(greek_tokens)[1]\n",
    "\n",
    "        # Concatenate the outputs\n",
    "        combined = torch.cat((outputs_linear_b, outputs_greek), 1)\n",
    "\n",
    "        # Pass through additional layers; placeholders\n",
    "        x = self.fc1(combined)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        #print x\n",
    "        # Should be tensor with logits\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHW72bz1S03Q"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pjRsqtmZUfW"
   },
   "source": [
    "## TRAINING\n",
    "\n",
    "- Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Up1TAf_STKbE"
   },
   "outputs": [],
   "source": [
    "unique_greek_tokens = set()\n",
    "\n",
    "for tokens in data_linearb_names_train['greek']:\n",
    "    unique_greek_tokens.update(tokens.split('|'))\n",
    "\n",
    "for tokens in data_linearb_names_test['greek']:\n",
    "    unique_greek_tokens.update(tokens.split('|'))\n",
    "\n",
    "token_to_id = {token: idx for idx, token in enumerate(unique_greek_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLn_2sABZZ7j"
   },
   "outputs": [],
   "source": [
    "class CognateDataset(Dataset):\n",
    "    def __init__(self, linear_b_tokens, greek_tokens, token_to_id, default_id=0):\n",
    "        self.linear_b_tokens = linear_b_tokens\n",
    "        self.greek_tokens = greek_tokens\n",
    "        self.token_to_id = token_to_id\n",
    "        self.default_id = default_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.linear_b_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        linear_b_token_tensor = torch.tensor(self.linear_b_tokens[idx], dtype=torch.long)\n",
    "        greek_token_tensor = torch.tensor(self.greek_tokens[idx], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'linear_b_tokens': linear_b_token_tensor,\n",
    "            'greek_tokens': greek_token_tensor\n",
    "        }\n",
    "\n",
    "train_dataset = CognateDataset(\n",
    "    data_linearb_names_train['linear_b_tokens'].tolist(),\n",
    "    data_linearb_names_train['greek_tokens'].tolist(),\n",
    "    token_to_id,\n",
    "    default_id=0\n",
    ")\n",
    "\n",
    "test_dataset = CognateDataset(\n",
    "    data_linearb_names_test['linear_b_tokens'].tolist(),\n",
    "    # For test data, you might not have labels or might handle them differently\n",
    "    [0] * len(data_linearb_names_test),  # Placeholder if you don't have labels\n",
    "    token_to_id,\n",
    "    default_id=0\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    linear_b_tokens = [item['linear_b_tokens'] for item in batch]\n",
    "    greek_tokens = [item['greek_tokens'] for item in batch]\n",
    "\n",
    "    # Pad sequences\n",
    "    linear_b_tokens_padded = pad_sequence(linear_b_tokens, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    greek_tokens_padded = pad_sequence(greek_tokens, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    return {\n",
    "        'linear_b_tokens': linear_b_tokens_padded,\n",
    "        'greek_tokens': greek_tokens_padded\n",
    "    }\n",
    "\n",
    "data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHyiuSm8Tpr9"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "model = CognatePredictionModel(bert_model)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        linear_b_tokens = batch['linear_b_tokens']\n",
    "        greek_tokens = batch['greek_tokens']\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(linear_b_tokens, greek_tokens)\n",
    "        outputs = outputs.squeeze()\n",
    "\n",
    "        loss = loss_function(outputs, greek_tokens.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels = (outputs > 0).float()\n",
    "        correct_predictions += (predicted_labels == greek_tokens).sum().item()\n",
    "        total_predictions += greek_tokens.numel()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCwEcUksZCCv"
   },
   "source": [
    "## EVALUATION\n",
    "\n",
    "- The primary goal metric is accuracy as compared to NeuroDecipher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fs1bKF0ZJJp"
   },
   "outputs": [],
   "source": [
    "# Evaluation code\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (test)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
