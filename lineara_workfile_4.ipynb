{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ns329APPQlk1"
   },
   "source": [
    "# Lost in Translation: Computational Approach to Linear A Decryption with LSTM and Transformer Models\n",
    "### *Team: Steven Lu, Georgiy Sekretaryuk, Oluwafemi*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68cUlXO-Q1PE"
   },
   "source": [
    "## OUTLINE\n",
    "\n",
    "Part 1 Goals:\n",
    "- replicate NeuroDecipher LSTM model with Linear B\n",
    "- apply NeuroDecipher NLP approaches in a transformer model\n",
    "- test different pre-training techniques and parameters to see how it influences the result\n",
    "\n",
    "Part 2 Goals:\n",
    "\n",
    "...TBD after Nov 13\n",
    "- Work with Linear A here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A95gI6tYhP19"
   },
   "source": [
    "## IMPORTS\n",
    "\n",
    "Import the necessary libraries for the project and define any additional configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "T3eQrjVocsjt",
    "outputId": "5f007507-1e9a-4379-cbd2-aa95e3baca43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (4.33.3)\n",
      "Requirement already satisfied: filelock in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (1.23.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (2.30.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transliterate in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (1.10.2)\n",
      "Requirement already satisfied: six>=1.1.0 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transliterate) (1.16.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentencepiece in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (0.1.99)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textdistance'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install sentencepiece\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#!pip install textdistance\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtextdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m levenshtein\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshutil\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textdistance'"
     ]
    }
   ],
   "source": [
    "# IMPORT THE LIBRARIES HERE\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install transliterate\n",
    "!pip install sentencepiece\n",
    "#!pip install textdistance\n",
    "from textdistance import levenshtein\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from transliterate import translit, get_available_language_codes\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, MBart50TokenizerFast\n",
    "import torch\n",
    "import sentencepiece\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2ovQjQW5V2Zn"
   },
   "outputs": [],
   "source": [
    "#setup for GDrive\n",
    "# #@title SELECT USER to mount the data drive according to its path in your drive\n",
    "# USER = 'Georgiy' #@param ['Georgiy', 'Steven', 'Oluwafemi']\n",
    "\n",
    "# #@title Mount GDrive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# #remove cache\n",
    "# !rm -rf \"/content/drive/MyDrive/NLP_266/__pycache__\"\n",
    "\n",
    "# #@title Set PATH to /data/ folder\n",
    "# PATHS = {}\n",
    "# PATHS['Georgiy'] = \"/content/drive/MyDrive/NLP_266\"\n",
    "# PATHS['Steven'] = \"/content/drive/Shareddrives/PathForSteven\"  # Replace with the actual path\n",
    "# PATHS['Oluwafemi'] = \"/content/drive/Shareddrives/PathForOluwafemi\"  # Replace with the actual path\n",
    "# PATH = PATHS[USER]\n",
    "\n",
    "# if PATH == \"\":\n",
    "#     raise ValueError(\"Enter your path to the shared data folder.\\nIt should start with 'content/drive/...' and end with '.../281 Final Project/data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nD6I0mNzQjqj"
   },
   "outputs": [],
   "source": [
    "# # Import Lin B from NeuroDecipher https://github.com/j-luo93/NeuroDecipher\n",
    "#only run this if the NeuroDecipher folder is empty\n",
    "# folder_path = 'NeuroDecipher'\n",
    "\n",
    "# if os.path.exists(folder_path):\n",
    "#    shutil.rmtree(folder_path)\n",
    "#    print(f\"The folder '{folder_path}' has been removed.\")\n",
    "# else:\n",
    "#    print(f\"The folder '{folder_path}' does not exist.\")\n",
    "\n",
    "# !git clone https://github.com/j-luo93/NeuroDecipher\n",
    "# !git submodule init && git submodule update\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !cd NeuroDecipher && pip install -r requirements.txt\n",
    "# !cd NeuroDecipher && pip install .\n",
    "# !cd NeuroDecipher/arglib && ls\n",
    "# !cd NeuroDecipher/editdistance && pip install .\n",
    "# !cd NeuroDecipher/arglib && pip install .\n",
    "# !cd NeuroDecipher/dev_misc && pip install -r requirements.txt\n",
    "# !cd NeuroDecipher/dev_misc && pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvdwnoAuQ7qi"
   },
   "source": [
    "## LOAD THE DATA\n",
    "\n",
    "Load the data from https://github.com/j-luo93/NeuroDecipher.\n",
    "\n",
    "Each .cog file is essentially a tsv file, where each column corresponds to the words in one language. Words in the same row are considered cognates. If for one word, there is no corresponding cognate in another language, '_' is used to fill the cell. If multiple cognates are available for the same word, '|' is used to separate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pms31bkmRAHd",
    "outputId": "31fe6d17-33ce-4c67-95b3-f6956a0d691f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Linear B Cognates before modifications:\n",
      "     linear_b              greek\n",
      "0      ğ€€ğ€ğ€ªğ€¦ğ€²          Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚\n",
      "1       ğ€€ğ€ğ€´ğ€µ     Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚|ÎµÎ¸Î¹Î¶Ï‰\n",
      "2       ğ€€ğ€…ğ€”ğ€ƒ      Î±Î´Î±Î¼Î±Î¿|Î±Î´Î±Î¼Î±Ï‚\n",
      "3       ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚|Î±Î´Î±Î¼ÎµÏ…Ï‚\n",
      "4      ğ€€ğ€…ğ€¨ğ€´ğ€          Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚\n",
      "..       ...                ...\n",
      "914     ğ†ğ€¯ğ€Šğ€’          Ï†Ï…ÏƒÎ¹Î±ÏÏ‡Î¿Ï‚\n",
      "915       ğ†ğ€³              Ï†Ï…Ï„ÎµÏ\n",
      "916     ğ†ğ€³ğ€ªğ€Š            Ï†Ï…Ï„ÎµÏÎ¹Î±\n",
      "917   ğ†ğˆğ€€ğ€ğ€©ğ€„       Ï†Ï…Î»Î¹Î±Ï‚Î±Î³ÏÎµÏ…Ï‚\n",
      "918       ğ‡ğ€œ             Ï†Ï„ÎµÎ½Î¿Î¹\n",
      "\n",
      "[919 rows x 2 columns]\n",
      "Loaded Linear B Names before modifications:\n",
      "     linear_b              greek\n",
      "0      ğ€€ğ€ğ€ªğ€¦ğ€²          Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚\n",
      "1       ğ€€ğ€ğ€´ğ€µ                  _\n",
      "2       ğ€€ğ€…ğ€”ğ€ƒ      Î±Î´Î±Î¼Î±Î¿|Î±Î´Î±Î¼Î±Ï‚\n",
      "3       ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚|Î±Î´Î±Î¼ÎµÏ…Ï‚\n",
      "4      ğ€€ğ€…ğ€¨ğ€´ğ€          Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚\n",
      "..       ...                ...\n",
      "914     ğ†ğ€¯ğ€Šğ€’          Ï†Ï…ÏƒÎ¹Î±ÏÏ‡Î¿Ï‚\n",
      "915       ğ†ğ€³                  _\n",
      "916     ğ†ğ€³ğ€ªğ€Š                  _\n",
      "917   ğ†ğˆğ€€ğ€ğ€©ğ€„       Ï†Ï…Î»Î¹Î±Ï‚Î±Î³ÏÎµÏ…Ï‚\n",
      "918       ğ‡ğ€œ                  _\n",
      "\n",
      "[919 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the data into a pandas DataFrame\n",
    "file_path = 'NeuroDecipher/data/linear_b-greek.cog'\n",
    "file_path_names = 'NeuroDecipher/data/linear_b-greek.names.cog'\n",
    "data_linearb = pd.read_csv(file_path, sep='\\t', header=0)\n",
    "data_linearb_names = pd.read_csv(file_path_names, sep='\\t', header=0)\n",
    "\n",
    "# Print data for testing\n",
    "print('Loaded Linear B Cognates before modifications:\\n', data_linearb)\n",
    "print('Loaded Linear B Names before modifications:\\n', data_linearb_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg7KzeerYJdV"
   },
   "source": [
    "## DATA MODIFICATION\n",
    "\n",
    "- Do we split the data into individual letters?\n",
    "\n",
    "- INstead of columns for cog 1 / cog 2, turn it into rows -> increases dataset size\n",
    "- turn empty rows into test/train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HdD7NquyYRzk",
    "outputId": "66abc283-72b0-4b91-c410-bb249ecf938f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0        1     2     3     4\n",
      "0  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     None  None  None  None\n",
      "1          _     None  None  None  None\n",
      "2     Î±Î´Î±Î¼Î±Î¿   Î±Î´Î±Î¼Î±Ï‚  None  None  None\n",
      "3  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚  Î±Î´Î±Î¼ÎµÏ…Ï‚  None  None  None\n",
      "4  Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚     None  None  None  None\n"
     ]
    }
   ],
   "source": [
    "# @title Modify the Data\n",
    "\n",
    "# LINEAR B COGNATES\n",
    "\n",
    "# Renaming the original greek column to track the original\n",
    "data_linearb.rename(columns={'greek': 'greek_original'}, inplace=True)\n",
    "# Split the 'Greek' col into 2\n",
    "split_columns = data_linearb['greek_original'].str.split('|', expand=True)\n",
    "\n",
    "# Assigning split cols\n",
    "data_linearb['greek_cog_1'] = split_columns[0]\n",
    "data_linearb['greek_cog_2'] = split_columns[1].fillna('')\n",
    "data_linearb['greek_cog_3'] = split_columns[2].fillna('')\n",
    "data_linearb['greek_cog_4'] = split_columns[3].fillna('')\n",
    "data_linearb['greek_cog_5'] = split_columns[4].fillna('')\n",
    "\n",
    "# LINEAR B NAMES\n",
    "\n",
    "data_linearb_names.rename(columns={'greek': 'greek_original'}, inplace=True)\n",
    "# Split the 'Greek' col into 2\n",
    "split_columns = data_linearb_names['greek_original'].str.split('|', expand=True)\n",
    "print(split_columns.head()) #max: 5\n",
    "# Assigning split cols\n",
    "data_linearb_names['greek_cog_1'] = split_columns[0]\n",
    "data_linearb_names['greek_cog_2'] = split_columns[1].fillna('')\n",
    "data_linearb_names['greek_cog_3'] = split_columns[2].fillna('')\n",
    "data_linearb_names['greek_cog_4'] = split_columns[3].fillna('')\n",
    "data_linearb_names['greek_cog_5'] = split_columns[4].fillna('')\n",
    "# Replace all _ with blank space\n",
    "data_linearb_names.replace('_', '', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert(len(data_linearb)==len(data_linearb_names))\n",
    "data_linearb_split=[]\n",
    "data_linearb_names_split=[]\n",
    "for i in range(len(data_linearb)):\n",
    "    #fill linear B\n",
    "    temp=[data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_1\"].iloc[i]]\n",
    "    data_linearb_split.append(temp)\n",
    "    if data_linearb[\"greek_cog_2\"].iloc[i]!=\"\":\n",
    "        data_linearb_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_2\"].iloc[i]])\n",
    "    if data_linearb[\"greek_cog_3\"].iloc[i]!=\"\":\n",
    "        data_linearb_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_3\"].iloc[i]])\n",
    "    if data_linearb[\"greek_cog_4\"].iloc[i]!=\"\":\n",
    "        data_linearb_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_4\"].iloc[i]])\n",
    "    if data_linearb[\"greek_cog_5\"].iloc[i]!=\"\":\n",
    "        data_linearb_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_5\"].iloc[i]])\n",
    "    #fill linear B names\n",
    "    temp=[data_linearb_names[\"linear_b\"].iloc[i],data_linearb_names[\"greek_cog_1\"].iloc[i]]\n",
    "    data_linearb_names_split.append(temp)\n",
    "    if data_linearb_names[\"greek_cog_2\"].iloc[i]!=\"\":\n",
    "        data_linearb_names_split.append([data_linearb_names[\"linear_b\"].iloc[i],data_linearb_names[\"greek_cog_2\"].iloc[i]])\n",
    "    if data_linearb_names[\"greek_cog_3\"].iloc[i]!=\"\":\n",
    "        data_linearb_names_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_3\"].iloc[i]])\n",
    "    if data_linearb_names[\"greek_cog_4\"].iloc[i]!=\"\":\n",
    "        data_linearb_names_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_4\"].iloc[i]])\n",
    "    if data_linearb_names[\"greek_cog_5\"].iloc[i]!=\"\":\n",
    "        data_linearb_names_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_5\"].iloc[i]])\n",
    "data_linearb_split=pd.DataFrame(data_linearb_split,columns=[\"linear_b\",\"greek\"])\n",
    "data_linearb_names_split=pd.DataFrame(data_linearb_names_split,columns=[\"linear_b\",\"greek\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 919 lines\n",
      "  linear_b     greek_original greek_cog_1 greek_cog_2 greek_cog_3 greek_cog_4   \n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²          Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚   Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚                                      \\\n",
      "1     ğ€€ğ€ğ€´ğ€µ     Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚|ÎµÎ¸Î¹Î¶Ï‰    Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚       ÎµÎ¸Î¹Î¶Ï‰                           \n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ      Î±Î´Î±Î¼Î±Î¿|Î±Î´Î±Î¼Î±Ï‚      Î±Î´Î±Î¼Î±Î¿      Î±Î´Î±Î¼Î±Ï‚                           \n",
      "3     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚|Î±Î´Î±Î¼ÎµÏ…Ï‚   Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚     Î±Î´Î±Î¼ÎµÏ…Ï‚                           \n",
      "4    ğ€€ğ€…ğ€¨ğ€´ğ€          Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚   Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚                                       \n",
      "\n",
      "  greek_cog_5  \n",
      "0              \n",
      "1              \n",
      "2              \n",
      "3              \n",
      "4               \n",
      "\n",
      "Split: 1429 lines\n",
      "  linear_b      greek\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚\n",
      "1     ğ€€ğ€ğ€´ğ€µ   Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚\n",
      "2     ğ€€ğ€ğ€´ğ€µ      ÎµÎ¸Î¹Î¶Ï‰\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿\n",
      "4     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚ \n",
      "\n",
      "Value: ğ€©ğ€º Split Count: [5] Original Count: 7\n",
      "Value: ğ€µğ€¥ğ€† Split Count: [5] Original Count: 6\n",
      "\n",
      "We have 917 matches.\n"
     ]
    }
   ],
   "source": [
    "# Counting Linear B original dataset and split dataset.\n",
    "\n",
    "print(\"Original:\",len(data_linearb),\"lines\")\n",
    "print(data_linearb.head(),'\\n')\n",
    "print(\"Split:\",len(data_linearb_split),\"lines\")\n",
    "print(data_linearb_split.head(),'\\n')\n",
    "\n",
    "##########################\n",
    "###### SANITY CHECK ######\n",
    "##########################\n",
    "\n",
    "data_linearb_split_count = data_linearb_split[\"linear_b\"].value_counts().reset_index()\n",
    "data_linearb_split_count.columns = [\"linear_b\", \"count\"]\n",
    "# print(data_linearb_split_count)\n",
    "\n",
    "# Count how many greek definitions each linear b value has in the original table, separeted by '|'\n",
    "data_linearb_count = data_linearb[\"greek_original\"].apply(lambda x: 0 if pd.isna(x) else (1 if '|' not in x else x.count('|') + 1))\n",
    "data_linearb_count = pd.DataFrame({\"linear_b\": data_linearb[\"linear_b\"], \"count\": data_linearb_count})\n",
    "# print(data_linearb_count)\n",
    "\n",
    "# The values should match. If they don't, print out the rows that don't match. Otherwise, print out the number of matches.\n",
    "match_count = 0\n",
    "for index, row in data_linearb_count.iterrows():\n",
    "    split_count = data_linearb_split_count[data_linearb_split_count['linear_b'] == row['linear_b']]['count'].values\n",
    "    original_count = row['count']\n",
    "    if split_count != original_count:\n",
    "        print(\"Value:\", row['linear_b'], \"Split Count:\", split_count, \"Original Count:\", original_count)\n",
    "    else:\n",
    "        match_count += 1\n",
    "print(f\"\\nWe have {match_count} matches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 919 lines\n",
      "  linear_b     greek_original greek_cog_1 greek_cog_2 greek_cog_3 greek_cog_4   \n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²          Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚   Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚                                      \\\n",
      "1     ğ€€ğ€ğ€´ğ€µ                                                                      \n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ      Î±Î´Î±Î¼Î±Î¿|Î±Î´Î±Î¼Î±Ï‚      Î±Î´Î±Î¼Î±Î¿      Î±Î´Î±Î¼Î±Ï‚                           \n",
      "3     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚|Î±Î´Î±Î¼ÎµÏ…Ï‚   Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚     Î±Î´Î±Î¼ÎµÏ…Ï‚                           \n",
      "4    ğ€€ğ€…ğ€¨ğ€´ğ€          Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚   Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚                                       \n",
      "\n",
      "  greek_cog_5  \n",
      "0              \n",
      "1              \n",
      "2              \n",
      "3              \n",
      "4              \n",
      "Split: 1069 lines\n",
      "  linear_b      greek\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚\n",
      "1     ğ€€ğ€ğ€´ğ€µ           \n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚\n",
      "4     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚\n",
      "\n",
      "We have 919 matches.\n"
     ]
    }
   ],
   "source": [
    "#Counting Linear B Names and split names.\n",
    "\n",
    "# print('\\n ------ LINEAR B NAMES -----\\n')\n",
    "print(\"Original:\",len(data_linearb_names),\"lines\")\n",
    "print(data_linearb_names.head())\n",
    "print(\"Split:\",len(data_linearb_names_split),\"lines\")\n",
    "print(data_linearb_names_split.head())\n",
    "\n",
    "##########################\n",
    "###### SANITY CHECK ######\n",
    "##########################\n",
    "\n",
    "# Count how many times each linear b value appears int he split.\n",
    "data_linearb_names_split_count = data_linearb_names_split[\"linear_b\"].value_counts().reset_index()\n",
    "data_linearb_names_split_count.columns = [\"linear_b\", \"count\"]\n",
    "# print(data_linearb_names_split_count)\n",
    "\n",
    "# Count how many greek definitions each linear b value has in the original table, separeted by '|'\n",
    "data_linearb_names_count = data_linearb_names[\"greek_original\"].apply(lambda x: 0 if pd.isna(x) else (1 if '|' not in x else x.count('|') + 1))\n",
    "data_linearb_names_count = pd.DataFrame({\"linear_b\": data_linearb_names[\"linear_b\"], \"count\": data_linearb_names_count})\n",
    "# print(data_linearb_names_count)\n",
    "\n",
    "# The values should match. If they don't, print out the rows that don't match. Otherwise, print out the number of matches.\n",
    "match_count = 0\n",
    "for index, row in data_linearb_names_count.iterrows():\n",
    "    split_count = data_linearb_names_split_count[data_linearb_names_split_count['linear_b'] == row['linear_b']]['count'].values\n",
    "    original_count = row['count']\n",
    "    if split_count != original_count:\n",
    "        print(\"Value:\", row['linear_b'], \"Split Count:\", split_count, \"Original Count:\", original_count)\n",
    "    else:\n",
    "        match_count += 1\n",
    "print(f\"\\nWe have {match_count} matches.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking unique values in each column:\n",
      "\n",
      "data_linearb:\n",
      "\n",
      "linear_b Unique Values: 919\n",
      "greek_original Unique Values: 918\n",
      "greek_cog_1 Unique Values: 918\n",
      "greek_cog_2 Unique Values: 388\n",
      "greek_cog_3 Unique Values: 87\n",
      "greek_cog_4 Unique Values: 28\n",
      "greek_cog_5 Unique Values: 7\n",
      "\n",
      "\n",
      "data_linearb_names:\n",
      "\n",
      "linear_b Unique Values: 919\n",
      "greek_original Unique Values: 456\n",
      "greek_cog_1 Unique Values: 456\n",
      "greek_cog_2 Unique Values: 131\n",
      "greek_cog_3 Unique Values: 16\n",
      "greek_cog_4 Unique Values: 5\n",
      "greek_cog_5 Unique Values: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking unique values in each column:\\n\")\n",
    "\n",
    "print(\"data_linearb:\\n\")\n",
    "for col in data_linearb.columns:\n",
    "    if not isinstance(data_linearb[col].iloc[0], list):\n",
    "        print(f\"{col} Unique Values:\", data_linearb[col].nunique())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"data_linearb_names:\\n\")\n",
    "for col in data_linearb_names.columns:\n",
    "    print(f\"{col} Unique Values:\", data_linearb_names[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO by Sunday:\\n\\n[DONE]- Cycle through greek translations of linear B with more than 2 translations (separeted by \\'|\\' separator), create n columns where n is max count of translations in the largest row\\n\\n[DONE]- INstead of columns for cog 1 - cog n, turn it into rows -> increases dataset size \\n[DONE]- TWEAK - see above \\n[DONE]- Determine which are names and not names\\n#done- see below\\n    - For each language, create an object. In the \\n      object store data for the alphabet and the universal syllabic translation.\\n\\n\\n#OLD:\\n# - Create a mapping dict for universal character embeddings for linear b and for greek\\n#     - Create a unersal syllable matrix\\n# - Map linear b to universal syllables (matrix)\\n# - Map greek to universal syllables (matrix)\\n\\n#NEW:\\n[DONE]- Transliterate Linear B - done\\n[DONE]- Transliterate Modern Greek - done\\n\\n- Cycle through each greek word. Find word with highest \"syllabic matching\" to linear B and use that word for the model.\\nğ€€ğ€‡ğ€ªğ€Šğ€ \\tÎ±Î½Î´ÏÎ¹Î±Î½Ï„ÎµÎ¹|Î±Î½Î´ÏÎ¹Î±Ï†Î¹|Î±Î½Î´ÏÎ¹Î¿Ï‚|Î±Î½ÎµÏ\\n\\n- turn empty rows into train/test\\n#Steven - done: see above\\n- Create a train/test split of 20/80 (50/50 distribution of names/not name cognates?)\\n\\n\\n- Identify separators for transliterated Linear B\\n  - Separators between characters; separators between words\\n\\n- UNKNOWN: separators for transliterated Greek\\n  - Q: How do we set up the model to predict this...\\n\\n- Model BART, T5\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO by Sunday:\n",
    "\n",
    "[DONE]- Cycle through greek translations of linear B with more than 2 translations (separeted by '|' separator), create n columns where n is max count of translations in the largest row\n",
    "\n",
    "[DONE]- INstead of columns for cog 1 - cog n, turn it into rows -> increases dataset size \n",
    "[DONE]- TWEAK - see above \n",
    "[DONE]- Determine which are names and not names\n",
    "#done- see below\n",
    "    - For each language, create an object. In the \n",
    "      object store data for the alphabet and the universal syllabic translation.\n",
    "\n",
    "\n",
    "#OLD:\n",
    "# - Create a mapping dict for universal character embeddings for linear b and for greek\n",
    "#     - Create a unersal syllable matrix\n",
    "# - Map linear b to universal syllables (matrix)\n",
    "# - Map greek to universal syllables (matrix)\n",
    "\n",
    "#NEW:\n",
    "[DONE]- Transliterate Linear B - done\n",
    "[DONE]- Transliterate Modern Greek - done\n",
    "\n",
    "- Cycle through each greek word. Find word with highest \"syllabic matching\" to linear B and use that word for the model.\n",
    "ğ€€ğ€‡ğ€ªğ€Šğ€ \tÎ±Î½Î´ÏÎ¹Î±Î½Ï„ÎµÎ¹|Î±Î½Î´ÏÎ¹Î±Ï†Î¹|Î±Î½Î´ÏÎ¹Î¿Ï‚|Î±Î½ÎµÏ\n",
    "\n",
    "- turn empty rows into train/test\n",
    "#Steven - done: see above\n",
    "- Create a train/test split of 20/80 (50/50 distribution of names/not name cognates?)\n",
    "\n",
    "\n",
    "- Identify separators for transliterated Linear B\n",
    "  - Separators between characters; separators between words\n",
    "\n",
    "- UNKNOWN: separators for transliterated Greek\n",
    "  - Q: How do we set up the model to predict this...\n",
    "\n",
    "- Model BART, T5\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeriqota\n",
      "a<SEP>e<SEP>ri<SEP>qo<SEP>ta\n"
     ]
    }
   ],
   "source": [
    "# linear b syllabic mapping from NeuroDecipher MIT paper\n",
    "\n",
    "linb2syl = {\n",
    "    u'ğ€€': 'a', u'ğ€': 'e', u'ğ€‚': 'i', u'ğ€ƒ': 'o', u'ğ€„': 'u', u'ğ€…': 'da', u'ğ€†': 'de', \n",
    "    u'ğ€‡': 'di', u'ğ€ˆ': 'do', u'ğ€‰': 'du', u'ğ€Š': 'ja', u'ğ€‹': 'je', u'ğ€': 'jo', \n",
    "    u'ğ€': 'ju', u'ğ€': 'ka', u'ğ€': 'ke', u'ğ€‘': 'ki', u'ğ€’': 'ko', u'ğ€“': 'ku', \n",
    "    u'ğ€”': 'ma', u'ğ€•': 'me', u'ğ€–': 'mi', u'ğ€—': 'mo', u'ğ€˜': 'mu', u'ğ€™': 'na', \n",
    "    u'ğ€š': 'ne', u'ğ€›': 'ni', u'ğ€œ': 'no', u'ğ€': 'nu', u'ğ€': 'pa', u'ğ€Ÿ': 'pe', \n",
    "    u'ğ€ ': 'pi', u'ğ€¡': 'po', u'ğ€¢': 'pu', u'ğ€£': 'qa', u'ğ€¤': 'qe', u'ğ€¥': 'qi', \n",
    "    u'ğ€¦': 'qo', u'ğ€¨': 'ra', u'ğ€©': 're', u'ğ€ª': 'ri', u'ğ€«': 'ro', u'ğ€¬': 'ru',\n",
    "    u'ğ€­': 'sa', u'ğ€®': 'se', u'ğ€¯': 'si', u'ğ€°': 'so', u'ğ€±': 'su', u'ğ€²': 'ta', \n",
    "    u'ğ€³': 'te', u'ğ€´': 'ti', u'ğ€µ': 'to', u'ğ€¶': 'tu', u'ğ€·': 'wa', u'ğ€¸': 'we', \n",
    "    u'ğ€¹': 'wi', u'ğ€º': 'wo', u'ğ€¼': 'za', u'ğ€½': 'ze', u'ğ€¿': 'zo', u'ğ€': 'a2', \n",
    "    u'ğ': 'a3', u'ğ‚': 'au', u'ğƒ': 'dwe', u'ğ„': 'dwo', u'ğ…': 'nwa', u'ğ†': 'pu2', \n",
    "    u'ğ‡': 'pte', u'ğˆ': 'ra2', u'ğ‰': 'ra3', u'ğŠ': 'ro2', u'ğ‹': 'ta2', u'ğŒ': 'twe', u'ğ': 'two'\n",
    "}\n",
    "\n",
    "def transliterate_linb(word, dic, sep=None):\n",
    "    \"\"\"Transliterate Linear B characters into syllables.\"\"\"\n",
    "    \n",
    "    res=\"\" #result\n",
    "\n",
    "    # Cycle through each character in the Linear B word\n",
    "    for i, ch in enumerate(word):\n",
    "        # translation is the value for that character's key in the mapping dictionary\n",
    "        trans = dic[ch]\n",
    "        res += trans\n",
    "\n",
    "        # Add separator after each character except the last\n",
    "        if sep is not None and i < len(word) - 1:\n",
    "            res += sep\n",
    "\n",
    "    return res\n",
    "\n",
    "#test\n",
    "print(transliterate_linb(\"ğ€€ğ€ğ€ªğ€¦ğ€²\",linb2syl))\n",
    "print(transliterate_linb(\"ğ€€ğ€ğ€ªğ€¦ğ€²\",linb2syl,sep='<SEP>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current names dataset: 1069\n",
      "  linear_b      greek  Name\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1\n",
      "1     ğ€€ğ€ğ€´ğ€µ                0\n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚     1\n",
      "4     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚     1\n",
      "\n",
      "Current non-names dataset: 1429\n",
      "  linear_b      greek  Name\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1\n",
      "1     ğ€€ğ€ğ€´ğ€µ   Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚     0\n",
      "2     ğ€€ğ€ğ€´ğ€µ      ÎµÎ¸Î¹Î¶Ï‰     0\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1\n",
      "4     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚     1\n"
     ]
    }
   ],
   "source": [
    "#put binary inside the split name data of whether the linear b value is a name or not. \n",
    "\n",
    "def name(row):\n",
    "    if row[\"greek\"]==\"\":\n",
    "        return 0\n",
    "    return 1\n",
    "data_linearb_names_split[\"Name\"]=data_linearb_names_split.apply(name,axis=1)\n",
    "print(\"Current names dataset:\", len(data_linearb_names_split))\n",
    "print(data_linearb_names_split.head())\n",
    "\n",
    "#apply to the original, non-name dataset as well\n",
    "name_binary=[]\n",
    "for i in range(len(data_linearb_split)):\n",
    "    cur_linearb=data_linearb_split[\"linear_b\"].iloc[i]\n",
    "    corresponding_name=data_linearb_names_split[data_linearb_names_split[\"linear_b\"]==cur_linearb][\"greek\"].iloc[0]\n",
    "    if corresponding_name==\"\":\n",
    "        name_binary.append(0)\n",
    "    else: \n",
    "        name_binary.append(1)\n",
    "print(\"\\nCurrent non-names dataset:\", len(data_linearb_split))\n",
    "data_linearb_split[\"Name\"]=name_binary\n",
    "print(data_linearb_split.head())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transliterate Linear B Names \n",
    "\n",
    "# Create clean lists to store data\n",
    "greek_transliterate=[]\n",
    "greek_transliterate_names=[]\n",
    "from transliterate import translit, get_available_language_codes\n",
    "\n",
    "# Cycle through the dataset with the names\n",
    "for i in range(len(data_linearb_names_split)):\n",
    "    \n",
    "    #if blank, leave blank\n",
    "    if data_linearb_names_split[\"greek\"].iloc[i]==\"\":\n",
    "        greek_transliterate_names.append(\"\")\n",
    "    \n",
    "    #if not blank, transliterate\n",
    "    else:\n",
    "        greek_transliterate_names.append(translit(data_linearb_names_split[\"greek\"].iloc[i], reversed=True))\n",
    "\n",
    "# Cycle through the dataset with the non-names\n",
    "for i in range(len(data_linearb_split)):\n",
    "    \n",
    "    #if blank, leave blank\n",
    "    if data_linearb_split[\"greek\"].iloc[i]==\"\":\n",
    "        greek_transliterate.append(\"\")\n",
    "    \n",
    "    #if not blank, transliterate\n",
    "    else:\n",
    "        greek_transliterate.append(translit(data_linearb_split[\"greek\"].iloc[i], reversed=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transliterate Greek names\n",
    "\n",
    "# Create clean lists to store data\n",
    "linearb_transliterate=[]\n",
    "linearb_transliterate_names=[]\n",
    "\n",
    "# Cycle through the dataset with the names\n",
    "for i in range(len(data_linearb_names_split)):\n",
    "    \n",
    "    #if blank, leave blank\n",
    "    if data_linearb_names_split[\"linear_b\"].iloc[i]==\"\":linearb_transliterate_names.append(\"\")\n",
    "\n",
    "    #if not blank, transliterate\n",
    "    else:linearb_transliterate_names.append(transliterate_linb(data_linearb_names_split[\"linear_b\"].iloc[i], linb2syl))\n",
    "\n",
    "# Cycle through the dataset with the non-names\n",
    "for i in range(len(data_linearb_split)):\n",
    "    \n",
    "    #if blank, leave blank\n",
    "    if data_linearb_split[\"linear_b\"].iloc[i]==\"\":linearb_transliterate.append(\"\")\n",
    "    \n",
    "    #if not blank, transliterate\n",
    "    else:linearb_transliterate.append(transliterate_linb(data_linearb_split[\"linear_b\"].iloc[i], linb2syl))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1           aelipotas               aeriqota\n",
      "1     ğ€€ğ€ğ€´ğ€µ   Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚     0           aethistos                 aetito\n",
      "2     ğ€€ğ€ğ€´ğ€µ      ÎµÎ¸Î¹Î¶Ï‰     0              ethizo                 aetito\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1              adamao                 adamao\n",
      "4     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚     1              adamas                 adamao\n",
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1           aelipotas               aeriqota\n",
      "1     ğ€€ğ€ğ€´ğ€µ                0                                     aetito\n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1              adamao                 adamao\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚     1              adamas                 adamao\n",
      "4     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚     1           adamefeis                adamewe\n"
     ]
    }
   ],
   "source": [
    "assert(len(greek_transliterate)==len(data_linearb_split))\n",
    "assert(len(greek_transliterate_names)==len(data_linearb_names_split))\n",
    "data_linearb_split[\"greek_transliterate\"]=greek_transliterate\n",
    "data_linearb_names_split[\"greek_transliterate\"]=greek_transliterate_names\n",
    "data_linearb_split[\"linear_b_transliterate\"]=linearb_transliterate\n",
    "data_linearb_names_split[\"linear_b_transliterate\"]=linearb_transliterate_names\n",
    "print(data_linearb_split.head())\n",
    "print(data_linearb_names_split.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate   \n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1           aelipotas               aeriqota  \\\n",
      "1     ğ€€ğ€ğ€´ğ€µ   Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚     0           aethistos                 aetito   \n",
      "2     ğ€€ğ€ğ€´ğ€µ      ÎµÎ¸Î¹Î¶Ï‰     0              ethizo                 aetito   \n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1              adamao                 adamao   \n",
      "4     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚     1              adamas                 adamao   \n",
      "\n",
      "                linear_b_sep   linear_b_transliterated_sep  \n",
      "0  ğ€€<SEP>ğ€<SEP>ğ€ª<SEP>ğ€¦<SEP>ğ€²  a<SEP>e<SEP>ri<SEP>qo<SEP>ta  \n",
      "1        ğ€€<SEP>ğ€<SEP>ğ€´<SEP>ğ€µ         a<SEP>e<SEP>ti<SEP>to  \n",
      "2        ğ€€<SEP>ğ€<SEP>ğ€´<SEP>ğ€µ         a<SEP>e<SEP>ti<SEP>to  \n",
      "3        ğ€€<SEP>ğ€…<SEP>ğ€”<SEP>ğ€ƒ         a<SEP>da<SEP>ma<SEP>o  \n",
      "4        ğ€€<SEP>ğ€…<SEP>ğ€”<SEP>ğ€ƒ         a<SEP>da<SEP>ma<SEP>o  \n",
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate   \n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1           aelipotas               aeriqota  \\\n",
      "1     ğ€€ğ€ğ€´ğ€µ                0                                     aetito   \n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1              adamao                 adamao   \n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚     1              adamas                 adamao   \n",
      "4     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚     1           adamefeis                adamewe   \n",
      "\n",
      "                linear_b_sep   linear_b_transliterated_sep  \n",
      "0  ğ€€<SEP>ğ€<SEP>ğ€ª<SEP>ğ€¦<SEP>ğ€²  a<SEP>e<SEP>ri<SEP>qo<SEP>ta  \n",
      "1        ğ€€<SEP>ğ€<SEP>ğ€´<SEP>ğ€µ         a<SEP>e<SEP>ti<SEP>to  \n",
      "2        ğ€€<SEP>ğ€…<SEP>ğ€”<SEP>ğ€ƒ         a<SEP>da<SEP>ma<SEP>o  \n",
      "3        ğ€€<SEP>ğ€…<SEP>ğ€”<SEP>ğ€ƒ         a<SEP>da<SEP>ma<SEP>o  \n",
      "4        ğ€€<SEP>ğ€…<SEP>ğ€•<SEP>ğ€¸        a<SEP>da<SEP>me<SEP>we  \n"
     ]
    }
   ],
   "source": [
    "# Introduce separators for Linear B\n",
    "\n",
    "data_linearb_split['linear_b_sep'] = data_linearb_split['linear_b'].apply(lambda x: '<SEP>'.join(list(x)))\n",
    "data_linearb_names_split['linear_b_sep'] = data_linearb_names_split['linear_b'].apply(lambda x: '<SEP>'.join(list(x)))\n",
    "\n",
    "data_linearb_split['linear_b_transliterated_sep'] = data_linearb_split['linear_b'].apply(lambda x: transliterate_linb(x, linb2syl, sep='<SEP>'))\n",
    "data_linearb_names_split['linear_b_transliterated_sep'] = data_linearb_names_split['linear_b'].apply(lambda x: transliterate_linb(x, linb2syl, sep='<SEP>'))\n",
    "\n",
    "print(data_linearb_split.head())\n",
    "print(data_linearb_names_split.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn1-3Fg_Xyws"
   },
   "source": [
    "## EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "Analyze the dataset features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DlKUCwmpX5zR",
    "outputId": "b2a34baa-18a4-4f21-dd4f-c838c4de7690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- DESCRIBING THE NON-NAME DATA: -----\n",
      "\n",
      "              Name\n",
      "count  1429.000000\n",
      "mean      0.433170\n",
      "std       0.495687\n",
      "min       0.000000\n",
      "25%       0.000000\n",
      "50%       0.000000\n",
      "75%       1.000000\n",
      "max       1.000000\n",
      "\n",
      "----- INFO: -----\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1429 entries, 0 to 1428\n",
      "Data columns (total 7 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   linear_b                     1429 non-null   object\n",
      " 1   greek                        1429 non-null   object\n",
      " 2   Name                         1429 non-null   int64 \n",
      " 3   greek_transliterate          1429 non-null   object\n",
      " 4   linear_b_transliterate       1429 non-null   object\n",
      " 5   linear_b_sep                 1429 non-null   object\n",
      " 6   linear_b_transliterated_sep  1429 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 78.3+ KB\n",
      "None\n",
      "\n",
      "----- CHECKING FOR MISSING VALUES: -----\n",
      "\n",
      "linear_b                       0\n",
      "greek                          0\n",
      "Name                           0\n",
      "greek_transliterate            0\n",
      "linear_b_transliterate         0\n",
      "linear_b_sep                   0\n",
      "linear_b_transliterated_sep    0\n",
      "dtype: int64\n",
      "\n",
      "----- CHECKING UNIQUE VALUES: -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics and exploration\n",
    "\n",
    "print('\\n----- DESCRIBING THE NON-NAME DATA: -----\\n')\n",
    "print(data_linearb_split.describe())\n",
    "\n",
    "print('\\n----- INFO: -----\\n')\n",
    "print(data_linearb_split.info())\n",
    "\n",
    "# Check for missing values\n",
    "print('\\n----- CHECKING FOR MISSING VALUES: -----\\n')\n",
    "print(data_linearb_split.isnull().sum())\n",
    "\n",
    "# Explore unique values and frequency distribution\n",
    "print('\\n----- CHECKING UNIQUE VALUES: -----\\n')\n",
    "# print(data_linearb_split['linear_b'].value_counts())\n",
    "# print(data_linearb['greek_original'].value_counts())\n",
    "# print(data_linearb['greek_cog_1'].value_counts())\n",
    "# print(data_linearb['greek_cog_2'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EOtKu8kMS03N",
    "outputId": "e35b3c79-63f0-48be-ad06-50ff082cdbef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- DESCRIBING THE NAMES DATA: -----\n",
      "\n",
      "              Name\n",
      "count  1069.000000\n",
      "mean      0.565949\n",
      "std       0.495864\n",
      "min       0.000000\n",
      "25%       0.000000\n",
      "50%       1.000000\n",
      "75%       1.000000\n",
      "max       1.000000\n",
      "\n",
      "----- INFO: -----\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1069 entries, 0 to 1068\n",
      "Data columns (total 7 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   linear_b                     1069 non-null   object\n",
      " 1   greek                        1069 non-null   object\n",
      " 2   Name                         1069 non-null   int64 \n",
      " 3   greek_transliterate          1069 non-null   object\n",
      " 4   linear_b_transliterate       1069 non-null   object\n",
      " 5   linear_b_sep                 1069 non-null   object\n",
      " 6   linear_b_transliterated_sep  1069 non-null   object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 58.6+ KB\n",
      "None\n",
      "\n",
      "----- CHECKING FOR MISSING VALUES: -----\n",
      "\n",
      "linear_b                       0\n",
      "greek                          0\n",
      "Name                           0\n",
      "greek_transliterate            0\n",
      "linear_b_transliterate         0\n",
      "linear_b_sep                   0\n",
      "linear_b_transliterated_sep    0\n",
      "dtype: int64\n",
      "\n",
      "----- CHECKING UNIQUE VALUES: -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n----- DESCRIBING THE NAMES DATA: -----\\n')\n",
    "print(data_linearb_names_split.describe())\n",
    "\n",
    "print('\\n----- INFO: -----\\n')\n",
    "print(data_linearb_names_split.info())\n",
    "\n",
    "# Check for missing values\n",
    "print('\\n----- CHECKING FOR MISSING VALUES: -----\\n')\n",
    "print(data_linearb_names_split.isnull().sum())\n",
    "\n",
    "# Explore unique values and frequency distribution\n",
    "print('\\n----- CHECKING UNIQUE VALUES: -----\\n')\n",
    "# print(data_linearb_names_split['linear_b'].value_counts())\n",
    "# print(data_linearb_names_split['greek_original'].value_counts())\n",
    "# print(data_linearb_names_split['greek_cog_1'].value_counts())\n",
    "# print(data_linearb_names_split['greek_cog_2'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFOJEmRTS03O"
   },
   "source": [
    "## SPLITTING & TOKENIZATION\n",
    "\n",
    "- Create a train and test set.\n",
    "    - The train will contain the dataset where each Linear B name has a Greek name match.\n",
    "    - The test set will contain the dataset where Linear B has no Greek name match.\n",
    "- For each model, we need to create a separate tokenizer.\n",
    "    - We create a copy of the train/test set and tokenize for T5.\n",
    "    - We create a copy of the train/test set and tokenize for Bert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6UvDXgERS03M",
    "outputId": "532648f0-a511-442d-8eb7-84cd3f65a77d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linearb_names_train: 605\n",
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate   \n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1           aelipotas               aeriqota  \\\n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1              adamao                 adamao   \n",
      "\n",
      "                linear_b_sep   linear_b_transliterated_sep  \n",
      "0  ğ€€<SEP>ğ€<SEP>ğ€ª<SEP>ğ€¦<SEP>ğ€²  a<SEP>e<SEP>ri<SEP>qo<SEP>ta  \n",
      "2        ğ€€<SEP>ğ€…<SEP>ğ€”<SEP>ğ€ƒ         a<SEP>da<SEP>ma<SEP>o  \n",
      "linearb_names_test: 464\n",
      "  linear_b greek  Name greek_transliterate linear_b_transliterate   \n",
      "1     ğ€€ğ€ğ€´ğ€µ           0                                     aetito  \\\n",
      "7      ğ€€ğ€†ğ€³           0                                      adete   \n",
      "\n",
      "          linear_b_sep linear_b_transliterated_sep  \n",
      "1  ğ€€<SEP>ğ€<SEP>ğ€´<SEP>ğ€µ       a<SEP>e<SEP>ti<SEP>to  \n",
      "7        ğ€€<SEP>ğ€†<SEP>ğ€³             a<SEP>de<SEP>te  \n"
     ]
    }
   ],
   "source": [
    "#Creating the train/test split\n",
    "\n",
    "#only need to split names into train and test for now,\n",
    "#since the names has several hundred blanks while there are no blanks in the ovr data\n",
    "\n",
    "data_linearb_names_train=data_linearb_names_split[data_linearb_names_split[\"greek\"]!=\"\"]\n",
    "data_linearb_names_test=data_linearb_names_split[data_linearb_names_split[\"greek\"]==\"\"]\n",
    "print(\"linearb_names_train:\", len(data_linearb_names_train))\n",
    "print(data_linearb_names_train.head(2))\n",
    "print(\"linearb_names_test:\", len(data_linearb_names_test))\n",
    "print(data_linearb_names_test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize a column\n",
    "def tokenize_column(dataframe, column_name):\n",
    "    return dataframe[column_name].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization for T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# @Title: T5 Tokenizer\n",
    "\n",
    "# Copy datasets; separating tokenization for T5\n",
    "T5_data_linearb_names_train = data_linearb_names_train.copy()\n",
    "T5_data_linearb_names_test = data_linearb_names_test.copy()\n",
    "\n",
    "# Initialize T5 tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Tokenizing columns in the training data\n",
    "T5_data_linearb_names_train['linear_b_sep_tokenized'] = tokenize_column(T5_data_linearb_names_train, 'linear_b_sep')\n",
    "T5_data_linearb_names_train['linear_b_transliterated_sep_tokenized'] = tokenize_column(T5_data_linearb_names_train, 'linear_b_transliterated_sep')\n",
    "T5_data_linearb_names_train['linear_b_tokenized'] = tokenize_column(T5_data_linearb_names_train, 'linear_b')\n",
    "T5_data_linearb_names_train['greek_tokenized'] = tokenize_column(T5_data_linearb_names_train, 'greek')\n",
    "T5_data_linearb_names_train['greek_transliterate_tokenized'] = tokenize_column(T5_data_linearb_names_train, 'greek_transliterate')\n",
    "T5_data_linearb_names_train['linear_b_transliterate_tokenized'] = tokenize_column(T5_data_linearb_names_train, 'linear_b_transliterate')\n",
    "\n",
    "\n",
    "# Tokenizing columns in the test dataset\n",
    "T5_data_linearb_names_test['linear_b_sep_tokenized'] = tokenize_column(T5_data_linearb_names_test, 'linear_b_sep')\n",
    "T5_data_linearb_names_test['linear_b_transliterated_sep_tokenized'] = tokenize_column(T5_data_linearb_names_test, 'linear_b_transliterated_sep')\n",
    "T5_data_linearb_names_test['linear_b_tokenized'] = tokenize_column(T5_data_linearb_names_test, 'linear_b')\n",
    "T5_data_linearb_names_test['greek_tokenized'] = tokenize_column(T5_data_linearb_names_test, 'greek')\n",
    "T5_data_linearb_names_test['greek_transliterate_tokenized'] = tokenize_column(T5_data_linearb_names_test, 'greek_transliterate')\n",
    "T5_data_linearb_names_test['linear_b_transliterate_tokenized'] = tokenize_column(T5_data_linearb_names_test, 'linear_b_transliterate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   linear_b greek  Name greek_transliterate linear_b_transliterate   \n",
      "1      ğ€€ğ€ğ€´ğ€µ           0                                     aetito  \\\n",
      "7       ğ€€ğ€†ğ€³           0                                      adete   \n",
      "8     ğ€€ğ€‡ğ€ªğ€Šğ€            0                                  adirijapi   \n",
      "10     ğ€€ğ€Šğ€•ğ€™           0                                    ajamena   \n",
      "12     ğ€€ğ€ğ€¨ğ€œ           0                                    akarano   \n",
      "\n",
      "                 linear_b_sep    linear_b_transliterated_sep   \n",
      "1         ğ€€<SEP>ğ€<SEP>ğ€´<SEP>ğ€µ          a<SEP>e<SEP>ti<SEP>to  \\\n",
      "7               ğ€€<SEP>ğ€†<SEP>ğ€³                a<SEP>de<SEP>te   \n",
      "8   ğ€€<SEP>ğ€‡<SEP>ğ€ª<SEP>ğ€Š<SEP>ğ€   a<SEP>di<SEP>ri<SEP>ja<SEP>pi   \n",
      "10        ğ€€<SEP>ğ€Š<SEP>ğ€•<SEP>ğ€™         a<SEP>ja<SEP>me<SEP>na   \n",
      "12        ğ€€<SEP>ğ€<SEP>ğ€¨<SEP>ğ€œ         a<SEP>ka<SEP>ra<SEP>no   \n",
      "\n",
      "                               linear_b_sep_tokenized   \n",
      "1   [3, 2, 134, 8569, 3155, 2, 134, 8569, 3155, 2,...  \\\n",
      "7   [3, 2, 134, 8569, 3155, 2, 134, 8569, 3155, 2, 1]   \n",
      "8   [3, 2, 134, 8569, 3155, 2, 134, 8569, 3155, 2,...   \n",
      "10  [3, 2, 134, 8569, 3155, 2, 134, 8569, 3155, 2,...   \n",
      "12  [3, 2, 134, 8569, 3155, 2, 134, 8569, 3155, 2,...   \n",
      "\n",
      "                linear_b_transliterated_sep_tokenized linear_b_tokenized   \n",
      "1   [3, 9, 2, 134, 8569, 3155, 15, 2, 134, 8569, 3...          [3, 2, 1]  \\\n",
      "7   [3, 9, 2, 134, 8569, 3155, 221, 2, 134, 8569, ...          [3, 2, 1]   \n",
      "8   [3, 9, 2, 134, 8569, 3155, 26, 23, 2, 134, 856...          [3, 2, 1]   \n",
      "10  [3, 9, 2, 134, 8569, 3155, 1191, 2, 134, 8569,...          [3, 2, 1]   \n",
      "12  [3, 9, 2, 134, 8569, 3155, 1258, 2, 134, 8569,...          [3, 2, 1]   \n",
      "\n",
      "   greek_tokenized greek_transliterate_tokenized   \n",
      "1              [1]                           [1]  \\\n",
      "7              [1]                           [1]   \n",
      "8              [1]                           [1]   \n",
      "10             [1]                           [1]   \n",
      "12             [1]                           [1]   \n",
      "\n",
      "   linear_b_transliterate_tokenized  \n",
      "1        [3, 9, 15, 17, 23, 235, 1]  \n",
      "7            [3, 9, 221, 17, 15, 1]  \n",
      "8   [3, 9, 26, 23, 22276, 13306, 1]  \n",
      "10          [3, 9, 1191, 904, 9, 1]  \n",
      "12           [3, 5667, 2002, 32, 1]  \n"
     ]
    }
   ],
   "source": [
    "print(T5_data_linearb_names_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization for BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ChnA4BoES03O"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793aa01621f840ffb6224cd10b09f5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6836b9328c4ea5becc4bbff644e3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255ce65006384211a80efbc24e7789a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0006257dbcf74ca9b1b3172246b32106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# @title: BART TOKENIZER\n",
    "\n",
    "# Copy datasets; separating tokenization for BART\n",
    "BART_data_linearb_names_train = data_linearb_names_train.copy()\n",
    "BART_data_linearb_names_test = data_linearb_names_test.copy()\n",
    "\n",
    "#Initialize tokenizer\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50')\n",
    "\n",
    "# Tokenizing columns in the training data\n",
    "BART_data_linearb_names_train['linear_b_sep_tokenized'] = tokenize_column(BART_data_linearb_names_train, 'linear_b_sep')\n",
    "BART_data_linearb_names_train['linear_b_transliterated_sep_tokenized'] = tokenize_column(BART_data_linearb_names_train, 'linear_b_transliterated_sep')\n",
    "BART_data_linearb_names_train['linear_b_tokenized'] = tokenize_column(BART_data_linearb_names_train, 'linear_b')\n",
    "BART_data_linearb_names_train['greek_tokenized'] = tokenize_column(BART_data_linearb_names_train, 'greek')\n",
    "BART_data_linearb_names_train['greek_transliterate_tokenized'] = tokenize_column(BART_data_linearb_names_train, 'greek_transliterate')\n",
    "BART_data_linearb_names_train['linear_b_transliterate_tokenized'] = tokenize_column(BART_data_linearb_names_train, 'linear_b_transliterate')\n",
    "\n",
    "# Tokenizing columns in the test data\n",
    "BART_data_linearb_names_test['linear_b_sep_tokenized'] = tokenize_column(BART_data_linearb_names_test, 'linear_b_sep')\n",
    "BART_data_linearb_names_test['linear_b_transliterated_sep_tokenized'] = tokenize_column(BART_data_linearb_names_test, 'linear_b_transliterated_sep')\n",
    "BART_data_linearb_names_test['linear_b_tokenized'] = tokenize_column(BART_data_linearb_names_test, 'linear_b')\n",
    "BART_data_linearb_names_test['greek_tokenized'] = tokenize_column(BART_data_linearb_names_test, 'greek')\n",
    "BART_data_linearb_names_test['greek_transliterate_tokenized'] = tokenize_column(BART_data_linearb_names_test, 'greek_transliterate')\n",
    "BART_data_linearb_names_test['linear_b_transliterate_tokenized'] = tokenize_column(BART_data_linearb_names_test, 'linear_b_transliterate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate   \n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1           aelipotas               aeriqota  \\\n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1              adamao                 adamao   \n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚     1              adamas                 adamao   \n",
      "4     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚     1           adamefeis                adamewe   \n",
      "5     ğ€€ğ€…ğ€•ğ€¸    Î±Î´Î±Î¼ÎµÏ…Ï‚     1             adameus                adamewe   \n",
      "\n",
      "                linear_b_sep   linear_b_transliterated_sep   \n",
      "0  ğ€€<SEP>ğ€<SEP>ğ€ª<SEP>ğ€¦<SEP>ğ€²  a<SEP>e<SEP>ri<SEP>qo<SEP>ta  \\\n",
      "2        ğ€€<SEP>ğ€…<SEP>ğ€”<SEP>ğ€ƒ         a<SEP>da<SEP>ma<SEP>o   \n",
      "3        ğ€€<SEP>ğ€…<SEP>ğ€”<SEP>ğ€ƒ         a<SEP>da<SEP>ma<SEP>o   \n",
      "4        ğ€€<SEP>ğ€…<SEP>ğ€•<SEP>ğ€¸        a<SEP>da<SEP>me<SEP>we   \n",
      "5        ğ€€<SEP>ğ€…<SEP>ğ€•<SEP>ğ€¸        a<SEP>da<SEP>me<SEP>we   \n",
      "\n",
      "                              linear_b_sep_tokenized   \n",
      "0  [250004, 6, 3, 16093, 294, 21290, 2740, 3, 160...  \\\n",
      "2  [250004, 6, 3, 16093, 294, 21290, 2740, 3, 160...   \n",
      "3  [250004, 6, 3, 16093, 294, 21290, 2740, 3, 160...   \n",
      "4  [250004, 6, 3, 16093, 294, 21290, 2740, 3, 160...   \n",
      "5  [250004, 6, 3, 16093, 294, 21290, 2740, 3, 160...   \n",
      "\n",
      "               linear_b_transliterated_sep_tokenized linear_b_tokenized   \n",
      "0  [250004, 10, 16093, 294, 21290, 2740, 13, 1609...  [250004, 6, 3, 2]  \\\n",
      "2  [250004, 10, 16093, 294, 21290, 2740, 85, 1609...  [250004, 6, 3, 2]   \n",
      "3  [250004, 10, 16093, 294, 21290, 2740, 85, 1609...  [250004, 6, 3, 2]   \n",
      "4  [250004, 10, 16093, 294, 21290, 2740, 85, 1609...  [250004, 6, 3, 2]   \n",
      "5  [250004, 10, 16093, 294, 21290, 2740, 85, 1609...  [250004, 6, 3, 2]   \n",
      "\n",
      "                                greek_tokenized   \n",
      "0  [250004, 5961, 22506, 1473, 35119, 76581, 2]  \\\n",
      "2          [250004, 5961, 15349, 4601, 1457, 2]   \n",
      "3              [250004, 72385, 46570, 22355, 2]   \n",
      "4         [250004, 72385, 33854, 420, 11243, 2]   \n",
      "5               [250004, 72385, 33854, 9341, 2]   \n",
      "\n",
      "       greek_transliterate_tokenized   linear_b_transliterate_tokenized  \n",
      "0  [250004, 10, 13, 112114, 1073, 2]  [250004, 10, 4207, 10618, 102, 2]  \n",
      "2         [250004, 10, 18898, 31, 2]         [250004, 10, 18898, 31, 2]  \n",
      "3             [250004, 10, 36999, 2]         [250004, 10, 18898, 31, 2]  \n",
      "4   [250004, 1243, 282, 51899, 7, 2]       [250004, 1243, 282, 1177, 2]  \n",
      "5          [250004, 31786, 19005, 2]       [250004, 1243, 282, 1177, 2]  \n"
     ]
    }
   ],
   "source": [
    "print(BART_data_linearb_names_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview of Tokenized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5_data_linearb_names_train: 605\n",
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate   \n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1           aelipotas               aeriqota  \\\n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1              adamao                 adamao   \n",
      "\n",
      "                linear_b_sep   linear_b_transliterated_sep   \n",
      "0  ğ€€<SEP>ğ€<SEP>ğ€ª<SEP>ğ€¦<SEP>ğ€²  a<SEP>e<SEP>ri<SEP>qo<SEP>ta  \\\n",
      "2        ğ€€<SEP>ğ€…<SEP>ğ€”<SEP>ğ€ƒ         a<SEP>da<SEP>ma<SEP>o   \n",
      "\n",
      "                              linear_b_sep_tokenized   \n",
      "0  [3, 2, 134, 8569, 3155, 2, 134, 8569, 3155, 2,...  \\\n",
      "2  [3, 2, 134, 8569, 3155, 2, 134, 8569, 3155, 2,...   \n",
      "\n",
      "               linear_b_transliterated_sep_tokenized linear_b_tokenized   \n",
      "0  [3, 9, 2, 134, 8569, 3155, 15, 2, 134, 8569, 3...          [3, 2, 1]  \\\n",
      "2  [3, 9, 2, 134, 8569, 3155, 26, 9, 2, 134, 8569...          [3, 2, 1]   \n",
      "\n",
      "  greek_tokenized      greek_transliterate_tokenized   \n",
      "0       [3, 2, 1]  [3, 9, 15, 40, 23, 3013, 9, 7, 1]  \\\n",
      "2       [3, 2, 1]             [3, 9, 7812, 9, 32, 1]   \n",
      "\n",
      "     linear_b_transliterate_tokenized  \n",
      "0  [3, 9, 49, 23, 1824, 32, 17, 9, 1]  \n",
      "2              [3, 9, 7812, 9, 32, 1]  \n",
      "\n",
      "T5_data_linearb_names_test: 464\n",
      "  linear_b greek  Name greek_transliterate linear_b_transliterate   \n",
      "1     ğ€€ğ€ğ€´ğ€µ           0                                     aetito  \\\n",
      "7      ğ€€ğ€†ğ€³           0                                      adete   \n",
      "\n",
      "          linear_b_sep linear_b_transliterated_sep   \n",
      "1  ğ€€<SEP>ğ€<SEP>ğ€´<SEP>ğ€µ       a<SEP>e<SEP>ti<SEP>to  \\\n",
      "7        ğ€€<SEP>ğ€†<SEP>ğ€³             a<SEP>de<SEP>te   \n",
      "\n",
      "                              linear_b_sep_tokenized   \n",
      "1  [3, 2, 134, 8569, 3155, 2, 134, 8569, 3155, 2,...  \\\n",
      "7  [3, 2, 134, 8569, 3155, 2, 134, 8569, 3155, 2, 1]   \n",
      "\n",
      "               linear_b_transliterated_sep_tokenized linear_b_tokenized   \n",
      "1  [3, 9, 2, 134, 8569, 3155, 15, 2, 134, 8569, 3...          [3, 2, 1]  \\\n",
      "7  [3, 9, 2, 134, 8569, 3155, 221, 2, 134, 8569, ...          [3, 2, 1]   \n",
      "\n",
      "  greek_tokenized greek_transliterate_tokenized   \n",
      "1             [1]                           [1]  \\\n",
      "7             [1]                           [1]   \n",
      "\n",
      "  linear_b_transliterate_tokenized  \n",
      "1       [3, 9, 15, 17, 23, 235, 1]  \n",
      "7           [3, 9, 221, 17, 15, 1]  \n",
      "\n",
      "BART_data_linearb_names_train: 605\n",
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate   \n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1           aelipotas               aeriqota  \\\n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1              adamao                 adamao   \n",
      "\n",
      "                linear_b_sep   linear_b_transliterated_sep   \n",
      "0  ğ€€<SEP>ğ€<SEP>ğ€ª<SEP>ğ€¦<SEP>ğ€²  a<SEP>e<SEP>ri<SEP>qo<SEP>ta  \\\n",
      "2        ğ€€<SEP>ğ€…<SEP>ğ€”<SEP>ğ€ƒ         a<SEP>da<SEP>ma<SEP>o   \n",
      "\n",
      "                              linear_b_sep_tokenized   \n",
      "0  [250004, 6, 3, 16093, 294, 21290, 2740, 3, 160...  \\\n",
      "2  [250004, 6, 3, 16093, 294, 21290, 2740, 3, 160...   \n",
      "\n",
      "               linear_b_transliterated_sep_tokenized linear_b_tokenized   \n",
      "0  [250004, 10, 16093, 294, 21290, 2740, 13, 1609...  [250004, 6, 3, 2]  \\\n",
      "2  [250004, 10, 16093, 294, 21290, 2740, 85, 1609...  [250004, 6, 3, 2]   \n",
      "\n",
      "                                greek_tokenized   \n",
      "0  [250004, 5961, 22506, 1473, 35119, 76581, 2]  \\\n",
      "2          [250004, 5961, 15349, 4601, 1457, 2]   \n",
      "\n",
      "       greek_transliterate_tokenized   linear_b_transliterate_tokenized  \n",
      "0  [250004, 10, 13, 112114, 1073, 2]  [250004, 10, 4207, 10618, 102, 2]  \n",
      "2         [250004, 10, 18898, 31, 2]         [250004, 10, 18898, 31, 2]  \n",
      "\n",
      "BART_data_linearb_names_test: 464\n",
      "  linear_b greek  Name greek_transliterate linear_b_transliterate   \n",
      "1     ğ€€ğ€ğ€´ğ€µ           0                                     aetito  \\\n",
      "7      ğ€€ğ€†ğ€³           0                                      adete   \n",
      "\n",
      "          linear_b_sep linear_b_transliterated_sep   \n",
      "1  ğ€€<SEP>ğ€<SEP>ğ€´<SEP>ğ€µ       a<SEP>e<SEP>ti<SEP>to  \\\n",
      "7        ğ€€<SEP>ğ€†<SEP>ğ€³             a<SEP>de<SEP>te   \n",
      "\n",
      "                              linear_b_sep_tokenized   \n",
      "1  [250004, 6, 3, 16093, 294, 21290, 2740, 3, 160...  \\\n",
      "7  [250004, 6, 3, 16093, 294, 21290, 2740, 3, 160...   \n",
      "\n",
      "               linear_b_transliterated_sep_tokenized linear_b_tokenized   \n",
      "1  [250004, 10, 16093, 294, 21290, 2740, 13, 1609...  [250004, 6, 3, 2]  \\\n",
      "7  [250004, 10, 16093, 294, 21290, 2740, 112, 160...  [250004, 6, 3, 2]   \n",
      "\n",
      "  greek_tokenized greek_transliterate_tokenized   \n",
      "1     [250004, 2]                   [250004, 2]  \\\n",
      "7     [250004, 2]                   [250004, 2]   \n",
      "\n",
      "  linear_b_transliterate_tokenized  \n",
      "1       [250004, 10, 126, 2986, 2]  \n",
      "7           [250004, 45737, 13, 2]  \n"
     ]
    }
   ],
   "source": [
    "print(\"T5_data_linearb_names_train:\", len(T5_data_linearb_names_train))\n",
    "print(T5_data_linearb_names_train.head(2))\n",
    "print(\"\\nT5_data_linearb_names_test:\", len(T5_data_linearb_names_test))\n",
    "print(T5_data_linearb_names_test.head(2))\n",
    "\n",
    "print(\"\\nBART_data_linearb_names_train:\", len(BART_data_linearb_names_train))\n",
    "print(BART_data_linearb_names_train.head(2))\n",
    "print(\"\\nBART_data_linearb_names_test:\", len(BART_data_linearb_names_test))\n",
    "print(BART_data_linearb_names_test.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hcpc27jJX7JU"
   },
   "source": [
    "## BUILDING THE MODEL\n",
    "\n",
    "The model's task is to learn how to map from a segmented input sequence to an unsegmented output sequence.\n",
    "\n",
    "Our goal is to explore how transformer models of different architecture will perform in deciphering a low resource language such as Linear B to modern Greek.\n",
    "\n",
    "**1. Baseline Model: T5**\n",
    "  \n",
    "- T5 was selected for its ability to handle a wide variety of NLP problems, including a wide range of text-to-text translation and sequencing. It uses an encoder-decoder structure (that we have not modified) and it's successful in a variety of tasks, including those where the relationship between input and output sequences is commplex.\n",
    "\n",
    "\n",
    "**2. BART**\n",
    "\n",
    "**3. MarianMT**\n",
    "\n",
    "**4. XLM-R**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. T5 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T5 Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Settings\n",
    "\n",
    "T5_lr = 0.0001\n",
    "T5_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/homebrew/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.077653803323445\n",
      "Epoch 1, Loss: 2.5536735026459945\n",
      "Epoch 2, Loss: 2.0838070260851005\n",
      "Epoch 3, Loss: 1.8795968231401945\n",
      "Epoch 4, Loss: 1.7991241718593396\n"
     ]
    }
   ],
   "source": [
    "#T5 Model\n",
    "\n",
    "class LinearBToGreekDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize input and label text\n",
    "        input_text = \"translate Linear B to Greek: \" + self.dataframe.iloc[idx]['linear_b_transliterated_sep']\n",
    "        target_text = self.dataframe.iloc[idx]['greek_transliterate']\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True).squeeze()\n",
    "        labels = self.tokenizer.encode(target_text, return_tensors=\"pt\", max_length=512, truncation=True).squeeze()\n",
    "        return input_ids, labels\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        input_ids, labels = zip(*batch)\n",
    "        input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        labels_padded = pad_sequence(labels, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        return input_ids_padded, labels_padded\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "train_dataset = LinearBToGreekDataset(T5_data_linearb_names_train, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=T5_lr)\n",
    "epoch_loss = []\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(T5_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    epoch_loss.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "train_dataset = LinearBToGreekDataset(T5_data_linearb_names_train, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "# Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "epoch_loss = []\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(T5_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    epoch_loss.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {avg_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting loss\n",
    "\n",
    "plt.plot(epoch_loss, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_texts = [\"translate LinearB to Greek: \" + str(entry) for entry in T5_data_linearb_names_test['linear_b_transliterated_sep']]\n",
    "test_inputs = tokenizer(test_input_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    test_inputs = {k: v.to(device) for k, v in test_inputs.items()}\n",
    "    test_summary_ids = model.generate(test_inputs['input_ids'], num_beams=4, no_repeat_ngram_size=2, max_length=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'levenshtein' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb Cell 44\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_scores\u001b[39m(word1,word2):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m levenshtein\u001b[39m.\u001b[39mdistance(word1,word2)\u001b[39m/\u001b[39m\u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(word2),\u001b[39mlen\u001b[39m(word1))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(similarity_scores(\u001b[39m\"\u001b[39;49m\u001b[39maetito\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39maethistos\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[1;32m/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb Cell 44\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_scores\u001b[39m(word1,word2):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m levenshtein\u001b[39m.\u001b[39mdistance(word1,word2)\u001b[39m/\u001b[39m\u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(word2),\u001b[39mlen\u001b[39m(word1))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'levenshtein' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to measure word similaritieis with Levenshtein\n",
    "\n",
    "#levenshtein distance\n",
    "#!pip install textdistance\n",
    "# from textdistance import levenshtein\n",
    "\n",
    "def similarity_scores(word1,word2):\n",
    "    return levenshtein.distance(word1,word2)/max(len(word2),len(word1))\n",
    "print(similarity_scores(\"aetito\",\"aethistos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aetito aethistos\n",
      "aetito ethizo\n",
      "adete andetir\n",
      "adete andetires\n",
      "adipi andriantei\n",
      "adipi andriafi\n",
      "adipi andrios\n",
      "adipi aner\n",
      "ajamena aiasmena\n",
      "ajamena aiasmenos\n",
      "akaranos akaranos\n",
      "               Original Linear B Prediction      Actual  Best Similarity Score\n",
      "0          a<SEP>e<SEP>ti<SEP>to     aetito   aethistos               0.500000\n",
      "1          a<SEP>e<SEP>ti<SEP>to     aetito      ethizo               0.500000\n",
      "2                a<SEP>de<SEP>te      adete     andetir               0.444444\n",
      "3                a<SEP>de<SEP>te      adete   andetires               0.444444\n",
      "4  a<SEP>di<SEP>ri<SEP>ja<SEP>pi      adipi  andriantei               0.800000\n",
      "0.5122179363846031\n"
     ]
    }
   ],
   "source": [
    "test_predictions = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in test_summary_ids]\n",
    "\n",
    "comparison_df=[]\n",
    "#for prediction in test_predictions:\n",
    "#    print(prediction)\n",
    "assert(len(test_inputs[\"input_ids\"])==len(test_predictions))\n",
    "\n",
    "#get linear b transliteration in the original non-names dataset, and in the input test dataset\n",
    "for i in range(len(test_predictions)):\n",
    "\n",
    "    cur_prediction_greek=test_predictions[i]\n",
    "    #print(cur_prediction_greek)\n",
    "    cur_split_linb_transliteration=T5_data_linearb_names_test[\"linear_b_transliterated_sep\"].iloc[i]\n",
    "    \n",
    "    #get the subset of the original non-names dataset which has the same linear_b transliteration\n",
    "    data_linearb_subset=data_linearb_split[data_linearb_split[\"linear_b_transliterated_sep\"]==cur_split_linb_transliteration]\n",
    "    if len(data_linearb_subset)==0:\n",
    "        print(i)\n",
    "    \n",
    "    #build up prediction vs. actual, and append the maximum score for the current input\n",
    "    scores=[]\n",
    "    indices=[]\n",
    "    for j in range(len(data_linearb_subset)):\n",
    "        #print(cur_prediction_greek,\"|\",data_linearb_subset[\"greek_transliterate\"].iloc[j])\n",
    "        comparison_df.append([cur_split_linb_transliteration,cur_prediction_greek,\\\n",
    "                              data_linearb_subset[\"greek_transliterate\"].iloc[j]])\n",
    "        if i<5:print(cur_prediction_greek,data_linearb_subset[\"greek_transliterate\"].iloc[j])\n",
    "        scores.append(similarity_scores(cur_prediction_greek,data_linearb_subset[\"greek_transliterate\"].iloc[j]))\n",
    "    best_score_for_subset=max(scores)\n",
    "    for lst in comparison_df:\n",
    "        if lst[0]!=cur_split_linb_transliteration:continue\n",
    "        lst.append(best_score_for_subset)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "comparison_df=pd.DataFrame(comparison_df,columns=[\"Original Linear B\",\"Prediction\",\"Actual\",\"Best Similarity Score\"])\n",
    "print(comparison_df.head())\n",
    "print(comparison_df[\"Best Similarity Score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "# t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "\n",
    "# input_texts = [\"translate LinearB to Greek: \" + str(entry) for entry in T5_data_linearb_names_train['linear_b_transliterated_sep']]\n",
    "# inputs = t5_tokenizer(input_texts, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# summary_ids = t5_model.generate(inputs['input_ids'],\n",
    "#                                 num_beams=3,\n",
    "#                                 no_repeat_ngram_size=3,\n",
    "#                                 min_length=10,\n",
    "#                                 max_length=40)\n",
    "\n",
    "# predictions = [t5_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
    "\n",
    "# for prediction in predictions:\n",
    "#     print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_texts = [\"translate LinearB to Greek: \" + str(entry) for entry in T5_data_linearb_names_test['linear_b_transliterated_sep']]\n",
    "# test_inputs = t5_tokenizer(input_texts, return_tensors='tf', padding=True, truncation=True)\n",
    "\n",
    "# summary_ids = t5_model.generate(test_inputs['input_ids'],\n",
    "#                                 num_beams=4,\n",
    "#                                 no_repeat_ngram_size=2,\n",
    "#                                 max_length=50)\n",
    "\n",
    "# predictions = [t5_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Models\n",
    "\n",
    "- XLM_R (Multilingual version of RoBERTa)\n",
    "- MarianMT (Seq2Sec)\n",
    "- mT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCwEcUksZCCv"
   },
   "source": [
    "## EVALUATION\n",
    "\n",
    "- Model performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fs1bKF0ZJJp"
   },
   "outputs": [],
   "source": [
    "# Evaluation code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BART MODEL\n",
    "\n",
    "# from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "# article_A = \"Input A\"\n",
    "# article_B = \"Expected Output B\"\n",
    "\n",
    "# model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "# tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# # translate Linear B to Greek\n",
    "# tokenizer.src_lang = \"ar_AR\"\n",
    "# encoded_ar = tokenizer(article_ar, return_tensors=\"pt\")\n",
    "# generated_tokens = model.generate(**encoded_ar, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n",
    "# tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove any existing file/directory with the same name\n",
    "# !rm -rf bart.large.tar.gz\n",
    "\n",
    "# # Download the BART model\n",
    "# !wget https://dl.fbaipublicfiles.com/fairseq/models/bart.large.tar.gz\n",
    "\n",
    "# # Extract the tar file\n",
    "# !tar -xzvf bart.large.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_directory = './bart.large.tar.gz'\n",
    "# bart = BARTModel.from_pretrained(model_directory, checkpoint_file='model.pt')\n",
    "# bart.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = bart.encode('Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚')\n",
    "# print(\"Encoded tokens:\", tokens.tolist())\n",
    "# decoded_text = bart.decode(tokens)\n",
    "# print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_layer_features = bart.extract_features(tokens)\n",
    "\n",
    "# assert last_layer_features.size() == torch.Size([1, len(tokens), bart.model.encoder.embed_tokens.embedding_dim])\n",
    "\n",
    "# all_layers = bart.extract_features(tokens, return_all_hiddens=True)\n",
    "\n",
    "# assert len(all_layers) == bart.model.encoder.layers.__len__() + 1  # +1 for the embedding layer\n",
    "# assert torch.all(all_layers[-1] == last_layer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03H5YWQ3S03Q"
   },
   "outputs": [],
   "source": [
    "# # Building the COgnate model (sample skeleton)\n",
    "\n",
    "# config = BertConfig.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "# bert_model = BertModel(config)\n",
    "\n",
    "# class CognatePredictionModel(nn.Module):\n",
    "#     def __init__(self, bert_model):\n",
    "#         super(CognatePredictionModel, self).__init__()\n",
    "#         self.bert = bert_model\n",
    "\n",
    "#         # BERT outputs a 768-d vector\n",
    "#         bert_output_size = 768\n",
    "\n",
    "#         # Additional fully connected layers\n",
    "#         self.fc1 = nn.Linear(bert_output_size * 2, 512)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "#         # Output layer for binary classification\n",
    "#         self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "#     def forward(self, linear_b_tokens, greek_tokens):\n",
    "#         # Pass input through BERT, take pooled output\n",
    "#         outputs_linear_b = self.bert(linear_b_tokens)[1]\n",
    "#         outputs_greek = self.bert(greek_tokens)[1]\n",
    "\n",
    "#         # Concatenate the outputs\n",
    "#         combined = torch.cat((outputs_linear_b, outputs_greek), 1)\n",
    "\n",
    "#         # Pass through additional layers; placeholders\n",
    "#         x = self.fc1(combined)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.fc3(x)\n",
    "\n",
    "#         #print x\n",
    "#         # Should be tensor with logits\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Up1TAf_STKbE"
   },
   "outputs": [],
   "source": [
    "# unique_greek_tokens = set()\n",
    "\n",
    "# for tokens in data_linearb_names_train['greek']:\n",
    "#     unique_greek_tokens.update(tokens.split('|'))\n",
    "\n",
    "# for tokens in data_linearb_names_test['greek']:\n",
    "#     unique_greek_tokens.update(tokens.split('|'))\n",
    "\n",
    "# token_to_id = {token: idx for idx, token in enumerate(unique_greek_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLn_2sABZZ7j"
   },
   "outputs": [],
   "source": [
    "# class CognateDataset(Dataset):\n",
    "#     def __init__(self, linear_b_tokens, greek_tokens, token_to_id, default_id=0):\n",
    "#         self.linear_b_tokens = linear_b_tokens\n",
    "#         self.greek_tokens = greek_tokens\n",
    "#         self.token_to_id = token_to_id\n",
    "#         self.default_id = default_id\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.linear_b_tokens)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         linear_b_token_tensor = torch.tensor(self.linear_b_tokens[idx], dtype=torch.long)\n",
    "#         greek_token_tensor = torch.tensor(self.greek_tokens[idx], dtype=torch.long)\n",
    "\n",
    "#         return {\n",
    "#             'linear_b_tokens': linear_b_token_tensor,\n",
    "#             'greek_tokens': greek_token_tensor\n",
    "#         }\n",
    "\n",
    "# train_dataset = CognateDataset(\n",
    "#     data_linearb_names_train['linear_b_tokens'].tolist(),\n",
    "#     data_linearb_names_train['greek_tokens'].tolist(),\n",
    "#     token_to_id,\n",
    "#     default_id=0\n",
    "# )\n",
    "\n",
    "# test_dataset = CognateDataset(\n",
    "#     data_linearb_names_test['linear_b_tokens'].tolist(),\n",
    "#     # For test data, you might not have labels or might handle them differently\n",
    "#     [0] * len(data_linearb_names_test),  # Placeholder if you don't have labels\n",
    "#     token_to_id,\n",
    "#     default_id=0\n",
    "# )\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     linear_b_tokens = [item['linear_b_tokens'] for item in batch]\n",
    "#     greek_tokens = [item['greek_tokens'] for item in batch]\n",
    "\n",
    "#     # Pad sequences\n",
    "#     linear_b_tokens_padded = pad_sequence(linear_b_tokens, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "#     greek_tokens_padded = pad_sequence(greek_tokens, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "#     return {\n",
    "#         'linear_b_tokens': linear_b_tokens_padded,\n",
    "#         'greek_tokens': greek_tokens_padded\n",
    "#     }\n",
    "\n",
    "# data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHyiuSm8Tpr9"
   },
   "outputs": [],
   "source": [
    "# # Model\n",
    "\n",
    "# model = CognatePredictionModel(bert_model)\n",
    "# loss_function = nn.BCEWithLogitsLoss()\n",
    "# optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# num_epochs = 5\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct_predictions = 0\n",
    "#     total_predictions = 0\n",
    "\n",
    "#     for batch in data_loader:\n",
    "#         linear_b_tokens = batch['linear_b_tokens']\n",
    "#         greek_tokens = batch['greek_tokens']\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(linear_b_tokens, greek_tokens)\n",
    "#         outputs = outputs.squeeze()\n",
    "\n",
    "#         loss = loss_function(outputs, greek_tokens.float())\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         predicted_labels = (outputs > 0).float()\n",
    "#         correct_predictions += (predicted_labels == greek_tokens).sum().item()\n",
    "#         total_predictions += greek_tokens.numel()\n",
    "\n",
    "#     avg_loss = total_loss / len(data_loader)\n",
    "#     accuracy = correct_predictions / total_predictions\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (test)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
