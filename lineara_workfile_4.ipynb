{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ns329APPQlk1"
   },
   "source": [
    "# Lost in Translation: Computational Approach to Linear A Decryption with LSTM and Transformer Models\n",
    "### *Team: Steven Lu, Georgiy Sekretaryuk, Oluwafemi*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68cUlXO-Q1PE"
   },
   "source": [
    "## OUTLINE\n",
    "\n",
    "Part 1 Goals:\n",
    "- replicate NeuroDecipher LSTM model with Linear B\n",
    "- apply NeuroDecipher NLP approaches in a transformer model\n",
    "- test different pre-training techniques and parameters to see how it influences the result\n",
    "\n",
    "Part 2 Goals:\n",
    "\n",
    "...TBD after Nov 13\n",
    "- Work with Linear A here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A95gI6tYhP19"
   },
   "source": [
    "## IMPORTS\n",
    "\n",
    "Import the necessary libraries for the project and define any additional configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "T3eQrjVocsjt",
    "outputId": "5f007507-1e9a-4379-cbd2-aa95e3baca43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (4.33.3)\n",
      "Requirement already satisfied: filelock in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (1.23.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (2.30.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: torch in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transliterate in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (1.10.2)\n",
      "Requirement already satisfied: six>=1.1.0 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transliterate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# IMPORT THE LIBRARIES HERE\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install transliterate\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2ovQjQW5V2Zn"
   },
   "outputs": [],
   "source": [
    "#setup for GDrive\n",
    "# #@title SELECT USER to mount the data drive according to its path in your drive\n",
    "# USER = 'Georgiy' #@param ['Georgiy', 'Steven', 'Oluwafemi']\n",
    "\n",
    "# #@title Mount GDrive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# #remove cache\n",
    "# !rm -rf \"/content/drive/MyDrive/NLP_266/__pycache__\"\n",
    "\n",
    "# #@title Set PATH to /data/ folder\n",
    "# PATHS = {}\n",
    "# PATHS['Georgiy'] = \"/content/drive/MyDrive/NLP_266\"\n",
    "# PATHS['Steven'] = \"/content/drive/Shareddrives/PathForSteven\"  # Replace with the actual path\n",
    "# PATHS['Oluwafemi'] = \"/content/drive/Shareddrives/PathForOluwafemi\"  # Replace with the actual path\n",
    "# PATH = PATHS[USER]\n",
    "\n",
    "# if PATH == \"\":\n",
    "#     raise ValueError(\"Enter your path to the shared data folder.\\nIt should start with 'content/drive/...' and end with '.../281 Final Project/data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nD6I0mNzQjqj"
   },
   "outputs": [],
   "source": [
    "# # Import Lin B from NeuroDecipher https://github.com/j-luo93/NeuroDecipher\n",
    "#only run this if the NeuroDecipher folder is empty\n",
    "# folder_path = 'NeuroDecipher'\n",
    "\n",
    "# if os.path.exists(folder_path):\n",
    "#    shutil.rmtree(folder_path)\n",
    "#    print(f\"The folder '{folder_path}' has been removed.\")\n",
    "# else:\n",
    "#    print(f\"The folder '{folder_path}' does not exist.\")\n",
    "\n",
    "# !git clone https://github.com/j-luo93/NeuroDecipher\n",
    "# !git submodule init && git submodule update\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !cd NeuroDecipher && pip install -r requirements.txt\n",
    "# !cd NeuroDecipher && pip install .\n",
    "# !cd NeuroDecipher/arglib && ls\n",
    "# !cd NeuroDecipher/editdistance && pip install .\n",
    "# !cd NeuroDecipher/arglib && pip install .\n",
    "# !cd NeuroDecipher/dev_misc && pip install -r requirements.txt\n",
    "# !cd NeuroDecipher/dev_misc && pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvdwnoAuQ7qi"
   },
   "source": [
    "## LOAD THE DATA\n",
    "\n",
    "Load the data from https://github.com/j-luo93/NeuroDecipher.\n",
    "\n",
    "Each .cog file is essentially a tsv file, where each column corresponds to the words in one language. Words in the same row are considered cognates. If for one word, there is no corresponding cognate in another language, _ is used to fill the cell. If multiple cognates are available for the same word, '|' is used to separate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pms31bkmRAHd",
    "outputId": "31fe6d17-33ce-4c67-95b3-f6956a0d691f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Linear B Cognates before modifications:\n",
      "     linear_b              greek\n",
      "0      ğ€€ğ€ğ€ªğ€¦ğ€²          Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚\n",
      "1       ğ€€ğ€ğ€´ğ€µ     Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚|ÎµÎ¸Î¹Î¶Ï‰\n",
      "2       ğ€€ğ€…ğ€”ğ€ƒ      Î±Î´Î±Î¼Î±Î¿|Î±Î´Î±Î¼Î±Ï‚\n",
      "3       ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚|Î±Î´Î±Î¼ÎµÏ…Ï‚\n",
      "4      ğ€€ğ€…ğ€¨ğ€´ğ€          Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚\n",
      "..       ...                ...\n",
      "914     ğ†ğ€¯ğ€Šğ€’          Ï†Ï…ÏƒÎ¹Î±ÏÏ‡Î¿Ï‚\n",
      "915       ğ†ğ€³              Ï†Ï…Ï„ÎµÏ\n",
      "916     ğ†ğ€³ğ€ªğ€Š            Ï†Ï…Ï„ÎµÏÎ¹Î±\n",
      "917   ğ†ğˆğ€€ğ€ğ€©ğ€„       Ï†Ï…Î»Î¹Î±Ï‚Î±Î³ÏÎµÏ…Ï‚\n",
      "918       ğ‡ğ€œ             Ï†Ï„ÎµÎ½Î¿Î¹\n",
      "\n",
      "[919 rows x 2 columns]\n",
      "Loaded Linear B Names before modifications:\n",
      "     linear_b              greek\n",
      "0      ğ€€ğ€ğ€ªğ€¦ğ€²          Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚\n",
      "1       ğ€€ğ€ğ€´ğ€µ                  _\n",
      "2       ğ€€ğ€…ğ€”ğ€ƒ      Î±Î´Î±Î¼Î±Î¿|Î±Î´Î±Î¼Î±Ï‚\n",
      "3       ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚|Î±Î´Î±Î¼ÎµÏ…Ï‚\n",
      "4      ğ€€ğ€…ğ€¨ğ€´ğ€          Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚\n",
      "..       ...                ...\n",
      "914     ğ†ğ€¯ğ€Šğ€’          Ï†Ï…ÏƒÎ¹Î±ÏÏ‡Î¿Ï‚\n",
      "915       ğ†ğ€³                  _\n",
      "916     ğ†ğ€³ğ€ªğ€Š                  _\n",
      "917   ğ†ğˆğ€€ğ€ğ€©ğ€„       Ï†Ï…Î»Î¹Î±Ï‚Î±Î³ÏÎµÏ…Ï‚\n",
      "918       ğ‡ğ€œ                  _\n",
      "\n",
      "[919 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the data into a pandas DataFrame\n",
    "file_path = 'NeuroDecipher/data/linear_b-greek.cog'\n",
    "file_path_names = 'NeuroDecipher/data/linear_b-greek.names.cog'\n",
    "data_linearb = pd.read_csv(file_path, sep='\\t', header=0)\n",
    "data_linearb_names = pd.read_csv(file_path_names, sep='\\t', header=0)\n",
    "\n",
    "print('Loaded Linear B Cognates before modifications:\\n', data_linearb)\n",
    "print('Loaded Linear B Names before modifications:\\n', data_linearb_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg7KzeerYJdV"
   },
   "source": [
    "## DATA MODIFICATION\n",
    "\n",
    "- Do we split the data into individual letters?\n",
    "\n",
    "- INstead of columns for cog 1 / cog 2, turn it into rows -> increases dataset size\n",
    "- turn empty rows into test/train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HdD7NquyYRzk",
    "outputId": "66abc283-72b0-4b91-c410-bb249ecf938f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0        1     2     3     4\n",
      "0  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     None  None  None  None\n",
      "1          _     None  None  None  None\n",
      "2     Î±Î´Î±Î¼Î±Î¿   Î±Î´Î±Î¼Î±Ï‚  None  None  None\n",
      "3  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚  Î±Î´Î±Î¼ÎµÏ…Ï‚  None  None  None\n",
      "4  Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚     None  None  None  None\n"
     ]
    }
   ],
   "source": [
    "# @title Modify the Data\n",
    "\n",
    "# LINEAR B COGNATES\n",
    "\n",
    "# Renaming the original greek column to track the original\n",
    "data_linearb.rename(columns={'greek': 'greek_original'}, inplace=True)\n",
    "# Split the 'Greek' col into 2\n",
    "split_columns = data_linearb['greek_original'].str.split('|', expand=True)\n",
    "\n",
    "# Assigning split cols\n",
    "data_linearb['greek_cog_1'] = split_columns[0]\n",
    "data_linearb['greek_cog_2'] = split_columns[1].fillna('')\n",
    "data_linearb['greek_cog_3'] = split_columns[2].fillna('')\n",
    "data_linearb['greek_cog_4'] = split_columns[3].fillna('')\n",
    "data_linearb['greek_cog_5'] = split_columns[4].fillna('')\n",
    "\n",
    "# LINEAR B NAMES\n",
    "\n",
    "data_linearb_names.rename(columns={'greek': 'greek_original'}, inplace=True)\n",
    "# Split the 'Greek' col into 2\n",
    "split_columns = data_linearb_names['greek_original'].str.split('|', expand=True)\n",
    "print(split_columns.head()) #max: 5\n",
    "# Assigning split cols\n",
    "data_linearb_names['greek_cog_1'] = split_columns[0]\n",
    "data_linearb_names['greek_cog_2'] = split_columns[1].fillna('')\n",
    "data_linearb_names['greek_cog_3'] = split_columns[2].fillna('')\n",
    "data_linearb_names['greek_cog_4'] = split_columns[3].fillna('')\n",
    "data_linearb_names['greek_cog_5'] = split_columns[4].fillna('')\n",
    "# Replace all _ with blank space\n",
    "data_linearb_names.replace('_', '', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 919 lines\n",
      "  linear_b     greek_original greek_cog_1 greek_cog_2 greek_cog_3 greek_cog_4   \n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²          Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚   Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚                                      \\\n",
      "1     ğ€€ğ€ğ€´ğ€µ     Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚|ÎµÎ¸Î¹Î¶Ï‰    Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚       ÎµÎ¸Î¹Î¶Ï‰                           \n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ      Î±Î´Î±Î¼Î±Î¿|Î±Î´Î±Î¼Î±Ï‚      Î±Î´Î±Î¼Î±Î¿      Î±Î´Î±Î¼Î±Ï‚                           \n",
      "3     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚|Î±Î´Î±Î¼ÎµÏ…Ï‚   Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚     Î±Î´Î±Î¼ÎµÏ…Ï‚                           \n",
      "4    ğ€€ğ€…ğ€¨ğ€´ğ€          Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚   Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚                                       \n",
      "\n",
      "  greek_cog_5  \n",
      "0              \n",
      "1              \n",
      "2              \n",
      "3              \n",
      "4               \n",
      "\n",
      "Split: 1429 lines\n",
      "  linear_b      greek\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚\n",
      "1     ğ€€ğ€ğ€´ğ€µ   Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚\n",
      "2     ğ€€ğ€ğ€´ğ€µ      ÎµÎ¸Î¹Î¶Ï‰\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿\n",
      "4     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert(len(data_linearb)==len(data_linearb_names))\n",
    "data_linearb_split=[]\n",
    "data_linearb_names_split=[]\n",
    "for i in range(len(data_linearb)):\n",
    "    #fill linear B\n",
    "    temp=[data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_1\"].iloc[i]]\n",
    "    data_linearb_split.append(temp)\n",
    "    if data_linearb[\"greek_cog_2\"].iloc[i]!=\"\":\n",
    "        data_linearb_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_2\"].iloc[i]])\n",
    "    if data_linearb[\"greek_cog_3\"].iloc[i]!=\"\":\n",
    "        data_linearb_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_3\"].iloc[i]])\n",
    "    if data_linearb[\"greek_cog_4\"].iloc[i]!=\"\":\n",
    "        data_linearb_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_4\"].iloc[i]])\n",
    "    if data_linearb[\"greek_cog_5\"].iloc[i]!=\"\":\n",
    "        data_linearb_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_5\"].iloc[i]])\n",
    "    #fill linear B names\n",
    "    temp=[data_linearb_names[\"linear_b\"].iloc[i],data_linearb_names[\"greek_cog_1\"].iloc[i]]\n",
    "    data_linearb_names_split.append(temp)\n",
    "    if data_linearb_names[\"greek_cog_2\"].iloc[i]!=\"\":\n",
    "        data_linearb_names_split.append([data_linearb_names[\"linear_b\"].iloc[i],data_linearb_names[\"greek_cog_2\"].iloc[i]])\n",
    "    if data_linearb_names[\"greek_cog_3\"].iloc[i]!=\"\":\n",
    "        data_linearb_names_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_3\"].iloc[i]])\n",
    "    if data_linearb_names[\"greek_cog_4\"].iloc[i]!=\"\":\n",
    "        data_linearb_names_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_4\"].iloc[i]])\n",
    "    if data_linearb_names[\"greek_cog_5\"].iloc[i]!=\"\":\n",
    "        data_linearb_names_split.append([data_linearb[\"linear_b\"].iloc[i],data_linearb[\"greek_cog_5\"].iloc[i]])\n",
    "data_linearb_split=pd.DataFrame(data_linearb_split,columns=[\"linear_b\",\"greek\"])\n",
    "data_linearb_names_split=pd.DataFrame(data_linearb_names_split,columns=[\"linear_b\",\"greek\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 919 lines\n",
      "  linear_b     greek_original greek_cog_1 greek_cog_2 greek_cog_3 greek_cog_4   \n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²          Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚   Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚                                      \\\n",
      "1     ğ€€ğ€ğ€´ğ€µ     Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚|ÎµÎ¸Î¹Î¶Ï‰    Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚       ÎµÎ¸Î¹Î¶Ï‰                           \n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ      Î±Î´Î±Î¼Î±Î¿|Î±Î´Î±Î¼Î±Ï‚      Î±Î´Î±Î¼Î±Î¿      Î±Î´Î±Î¼Î±Ï‚                           \n",
      "3     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚|Î±Î´Î±Î¼ÎµÏ…Ï‚   Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚     Î±Î´Î±Î¼ÎµÏ…Ï‚                           \n",
      "4    ğ€€ğ€…ğ€¨ğ€´ğ€          Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚   Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚                                       \n",
      "\n",
      "  greek_cog_5  \n",
      "0              \n",
      "1              \n",
      "2              \n",
      "3              \n",
      "4               \n",
      "\n",
      "Split: 1429 lines\n",
      "  linear_b      greek\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚\n",
      "1     ğ€€ğ€ğ€´ğ€µ   Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚\n",
      "2     ğ€€ğ€ğ€´ğ€µ      ÎµÎ¸Î¹Î¶Ï‰\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿\n",
      "4     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚ \n",
      "\n",
      "Value: ğ€©ğ€º Split Count: [5] Original Count: 7\n",
      "Value: ğ€µğ€¥ğ€† Split Count: [5] Original Count: 6\n",
      "\n",
      "We have 917 matches.\n"
     ]
    }
   ],
   "source": [
    "# Counting Linear B original dataset and split dataset.\n",
    "\n",
    "print(\"Original:\",len(data_linearb),\"lines\")\n",
    "print(data_linearb.head(),'\\n')\n",
    "print(\"Split:\",len(data_linearb_split),\"lines\")\n",
    "print(data_linearb_split.head(),'\\n')\n",
    "\n",
    "##########################\n",
    "###### SANITY CHECK ######\n",
    "##########################\n",
    "\n",
    "data_linearb_split_count = data_linearb_split[\"linear_b\"].value_counts().reset_index()\n",
    "data_linearb_split_count.columns = [\"linear_b\", \"count\"]\n",
    "# print(data_linearb_split_count)\n",
    "\n",
    "# Count how many greek definitions each linear b value has in the original table, separeted by '|'\n",
    "data_linearb_count = data_linearb[\"greek_original\"].apply(lambda x: 0 if pd.isna(x) else (1 if '|' not in x else x.count('|') + 1))\n",
    "data_linearb_count = pd.DataFrame({\"linear_b\": data_linearb[\"linear_b\"], \"count\": data_linearb_count})\n",
    "# print(data_linearb_count)\n",
    "\n",
    "# The values should match. If they don't, print out the rows that don't match. Otherwise, print out the number of matches.\n",
    "match_count = 0\n",
    "for index, row in data_linearb_count.iterrows():\n",
    "    split_count = data_linearb_split_count[data_linearb_split_count['linear_b'] == row['linear_b']]['count'].values\n",
    "    original_count = row['count']\n",
    "    if split_count != original_count:\n",
    "        print(\"Value:\", row['linear_b'], \"Split Count:\", split_count, \"Original Count:\", original_count)\n",
    "    else:\n",
    "        match_count += 1\n",
    "print(f\"\\nWe have {match_count} matches.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 919 lines\n",
      "  linear_b     greek_original greek_cog_1 greek_cog_2 greek_cog_3 greek_cog_4   \n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²          Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚   Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚                                      \\\n",
      "1     ğ€€ğ€ğ€´ğ€µ                                                                      \n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ      Î±Î´Î±Î¼Î±Î¿|Î±Î´Î±Î¼Î±Ï‚      Î±Î´Î±Î¼Î±Î¿      Î±Î´Î±Î¼Î±Ï‚                           \n",
      "3     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚|Î±Î´Î±Î¼ÎµÏ…Ï‚   Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚     Î±Î´Î±Î¼ÎµÏ…Ï‚                           \n",
      "4    ğ€€ğ€…ğ€¨ğ€´ğ€          Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚   Î±Î´ÏÎ±ÏƒÏ„Î¹Î¿Ï‚                                       \n",
      "\n",
      "  greek_cog_5  \n",
      "0              \n",
      "1              \n",
      "2              \n",
      "3              \n",
      "4              \n",
      "Split: 1069 lines\n",
      "  linear_b      greek\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚\n",
      "1     ğ€€ğ€ğ€´ğ€µ           \n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚\n",
      "4     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚\n",
      "\n",
      "We have 919 matches.\n"
     ]
    }
   ],
   "source": [
    "#Counting Linear B Names and split names.\n",
    "\n",
    "# print('\\n ------ LINEAR B NAMES -----\\n')\n",
    "print(\"Original:\",len(data_linearb_names),\"lines\")\n",
    "print(data_linearb_names.head())\n",
    "print(\"Split:\",len(data_linearb_names_split),\"lines\")\n",
    "print(data_linearb_names_split.head())\n",
    "\n",
    "##########################\n",
    "###### SANITY CHECK ######\n",
    "##########################\n",
    "\n",
    "# Count how many times each linear b value appears int he split.\n",
    "data_linearb_names_split_count = data_linearb_names_split[\"linear_b\"].value_counts().reset_index()\n",
    "data_linearb_names_split_count.columns = [\"linear_b\", \"count\"]\n",
    "# print(data_linearb_names_split_count)\n",
    "\n",
    "# Count how many greek definitions each linear b value has in the original table, separeted by '|'\n",
    "data_linearb_names_count = data_linearb_names[\"greek_original\"].apply(lambda x: 0 if pd.isna(x) else (1 if '|' not in x else x.count('|') + 1))\n",
    "data_linearb_names_count = pd.DataFrame({\"linear_b\": data_linearb_names[\"linear_b\"], \"count\": data_linearb_names_count})\n",
    "# print(data_linearb_names_count)\n",
    "\n",
    "# The values should match. If they don't, print out the rows that don't match. Otherwise, print out the number of matches.\n",
    "match_count = 0\n",
    "for index, row in data_linearb_names_count.iterrows():\n",
    "    split_count = data_linearb_names_split_count[data_linearb_names_split_count['linear_b'] == row['linear_b']]['count'].values\n",
    "    original_count = row['count']\n",
    "    if split_count != original_count:\n",
    "        print(\"Value:\", row['linear_b'], \"Split Count:\", split_count, \"Original Count:\", original_count)\n",
    "    else:\n",
    "        match_count += 1\n",
    "print(f\"\\nWe have {match_count} matches.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking unique values in each column:\n",
      "\n",
      "data_linearb:\n",
      "\n",
      "linear_b Unique Values: 919\n",
      "greek_original Unique Values: 918\n",
      "greek_cog_1 Unique Values: 918\n",
      "greek_cog_2 Unique Values: 388\n",
      "greek_cog_3 Unique Values: 87\n",
      "greek_cog_4 Unique Values: 28\n",
      "greek_cog_5 Unique Values: 7\n",
      "\n",
      "\n",
      "data_linearb_names:\n",
      "\n",
      "linear_b Unique Values: 919\n",
      "greek_original Unique Values: 456\n",
      "greek_cog_1 Unique Values: 456\n",
      "greek_cog_2 Unique Values: 131\n",
      "greek_cog_3 Unique Values: 16\n",
      "greek_cog_4 Unique Values: 5\n",
      "greek_cog_5 Unique Values: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking unique values in each column:\\n\")\n",
    "\n",
    "print(\"data_linearb:\\n\")\n",
    "for col in data_linearb.columns:\n",
    "    if not isinstance(data_linearb[col].iloc[0], list):\n",
    "        print(f\"{col} Unique Values:\", data_linearb[col].nunique())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"data_linearb_names:\\n\")\n",
    "for col in data_linearb_names.columns:\n",
    "    print(f\"{col} Unique Values:\", data_linearb_names[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "6UvDXgERS03M",
    "outputId": "532648f0-a511-442d-8eb7-84cd3f65a77d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605\n",
      "464\n"
     ]
    }
   ],
   "source": [
    "#only need to split names into train and test for now,\n",
    "#since the names has several hundred blanks while there are no blanks in the ovr data\n",
    "data_linearb_names_train=data_linearb_names_split[data_linearb_names_split[\"greek\"]!=\"\"]\n",
    "data_linearb_names_test=data_linearb_names_split[data_linearb_names_split[\"greek\"]==\"\"]\n",
    "print(len(data_linearb_names_train))\n",
    "print(len(data_linearb_names_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO by Sunday:\\n\\n- Cycle through greek translations of linear B with more than 2 translations (separeted by \\'|\\' separator), create n columns where n is max count of translations in the largest row\\n\\n- INstead of columns for cog 1 - cog n, turn it into rows -> increases dataset size \\n#- TWEAK - see above \\n- Determine which are names and not names\\n#done- see below\\n    - For each language, create an object. In the \\n      object store data for the alphabet and the universal syllabic translation.\\n\\n\\n#OLD:\\n# - Create a mapping dict for universal character embeddings for linear b and for greek\\n#     - Create a unersal syllable matrix\\n# - Map linear b to universal syllables (matrix)\\n# - Map greek to universal syllables (matrix)\\n\\n#NEW:\\n- Transliterate Linear B - done\\n- Transliterate Modern Greek - done\\n\\n- Cycle through each greek word. Find word with highest \"syllabic matching\" to linear B and use that word for the model.\\nğ€€ğ€‡ğ€ªğ€Šğ€ \\tÎ±Î½Î´ÏÎ¹Î±Î½Ï„ÎµÎ¹|Î±Î½Î´ÏÎ¹Î±Ï†Î¹|Î±Î½Î´ÏÎ¹Î¿Ï‚|Î±Î½ÎµÏ\\n\\n- turn empty rows into train/test\\n#Steven - done: see above\\n- Create a train/test split of 20/80 (50/50 distribution of names/not name cognates?)\\n\\n\\n- Identify separators for transliterated Linear B\\n  - Separators between characters; separators between words\\n\\n- UNKNOWN: separators for transliterated Greek\\n  - Q: How do we set up the model to predict this...\\n\\n- Model BART, T5\\n\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO by Sunday:\n",
    "\n",
    "[DONE]- Cycle through greek translations of linear B with more than 2 translations (separeted by '|' separator), create n columns where n is max count of translations in the largest row\n",
    "\n",
    "[DONE]- INstead of columns for cog 1 - cog n, turn it into rows -> increases dataset size \n",
    "[DONE]- TWEAK - see above \n",
    "[DONE]- Determine which are names and not names\n",
    "#done- see below\n",
    "    - For each language, create an object. In the \n",
    "      object store data for the alphabet and the universal syllabic translation.\n",
    "\n",
    "\n",
    "#OLD:\n",
    "# - Create a mapping dict for universal character embeddings for linear b and for greek\n",
    "#     - Create a unersal syllable matrix\n",
    "# - Map linear b to universal syllables (matrix)\n",
    "# - Map greek to universal syllables (matrix)\n",
    "\n",
    "#NEW:\n",
    "[DONE]- Transliterate Linear B - done\n",
    "[DONE]- Transliterate Modern Greek - done\n",
    "\n",
    "- Cycle through each greek word. Find word with highest \"syllabic matching\" to linear B and use that word for the model.\n",
    "ğ€€ğ€‡ğ€ªğ€Šğ€ \tÎ±Î½Î´ÏÎ¹Î±Î½Ï„ÎµÎ¹|Î±Î½Î´ÏÎ¹Î±Ï†Î¹|Î±Î½Î´ÏÎ¹Î¿Ï‚|Î±Î½ÎµÏ\n",
    "\n",
    "- turn empty rows into train/test\n",
    "#Steven - done: see above\n",
    "- Create a train/test split of 20/80 (50/50 distribution of names/not name cognates?)\n",
    "\n",
    "\n",
    "- Identify separators for transliterated Linear B\n",
    "  - Separators between characters; separators between words\n",
    "\n",
    "- UNKNOWN: separators for transliterated Greek\n",
    "  - Q: How do we set up the model to predict this...\n",
    "\n",
    "- Model BART, T5\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeriqota\n"
     ]
    }
   ],
   "source": [
    "linb2syl = {\n",
    "    u'ğ€€': 'a', u'ğ€': 'e', u'ğ€‚': 'i', u'ğ€ƒ': 'o', u'ğ€„': 'u', u'ğ€…': 'da', u'ğ€†': 'de', \n",
    "    u'ğ€‡': 'di', u'ğ€ˆ': 'do', u'ğ€‰': 'du', u'ğ€Š': 'ja', u'ğ€‹': 'je', u'ğ€': 'jo', \n",
    "    u'ğ€': 'ju', u'ğ€': 'ka', u'ğ€': 'ke', u'ğ€‘': 'ki', u'ğ€’': 'ko', u'ğ€“': 'ku', \n",
    "    u'ğ€”': 'ma', u'ğ€•': 'me', u'ğ€–': 'mi', u'ğ€—': 'mo', u'ğ€˜': 'mu', u'ğ€™': 'na', \n",
    "    u'ğ€š': 'ne', u'ğ€›': 'ni', u'ğ€œ': 'no', u'ğ€': 'nu', u'ğ€': 'pa', u'ğ€Ÿ': 'pe', \n",
    "    u'ğ€ ': 'pi', u'ğ€¡': 'po', u'ğ€¢': 'pu', u'ğ€£': 'qa', u'ğ€¤': 'qe', u'ğ€¥': 'qi', \n",
    "    u'ğ€¦': 'qo', u'ğ€¨': 'ra', u'ğ€©': 're', u'ğ€ª': 'ri', u'ğ€«': 'ro', u'ğ€¬': 'ru',\n",
    "    u'ğ€­': 'sa', u'ğ€®': 'se', u'ğ€¯': 'si', u'ğ€°': 'so', u'ğ€±': 'su', u'ğ€²': 'ta', \n",
    "    u'ğ€³': 'te', u'ğ€´': 'ti', u'ğ€µ': 'to', u'ğ€¶': 'tu', u'ğ€·': 'wa', u'ğ€¸': 'we', \n",
    "    u'ğ€¹': 'wi', u'ğ€º': 'wo', u'ğ€¼': 'za', u'ğ€½': 'ze', u'ğ€¿': 'zo', u'ğ€': 'a2', \n",
    "    u'ğ': 'a3', u'ğ‚': 'au', u'ğƒ': 'dwe', u'ğ„': 'dwo', u'ğ…': 'nwa', u'ğ†': 'pu2', \n",
    "    u'ğ‡': 'pte', u'ğˆ': 'ra2', u'ğ‰': 'ra3', u'ğŠ': 'ro2', u'ğ‹': 'ta2', u'ğŒ': 'twe', u'ğ': 'two'\n",
    "}\n",
    "\n",
    "def transliterate_linb(word,dic):\n",
    "    res=\"\"\n",
    "    for ch in word:\n",
    "        trans=dic[ch]\n",
    "        res+=trans\n",
    "    return res\n",
    "print(transliterate_linb(\"ğ€€ğ€ğ€ªğ€¦ğ€²\",linb2syl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current names dataset:\n",
      "  linear_b      greek  Name\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1\n",
      "1     ğ€€ğ€ğ€´ğ€µ                0\n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚     1\n",
      "4     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚     1\n",
      "Current non-names dataset:\n",
      "  linear_b      greek  Name\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1\n",
      "1     ğ€€ğ€ğ€´ğ€µ   Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚     0\n",
      "2     ğ€€ğ€ğ€´ğ€µ      ÎµÎ¸Î¹Î¶Ï‰     0\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1\n",
      "4     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚     1\n"
     ]
    }
   ],
   "source": [
    "#put binary inside the split name data of whether the linear b value is a name or not. \n",
    "\n",
    "def name(row):\n",
    "    if row[\"greek\"]==\"\":\n",
    "        return 0\n",
    "    return 1\n",
    "data_linearb_names_split[\"Name\"]=data_linearb_names_split.apply(name,axis=1)\n",
    "print(\"Current names dataset:\")\n",
    "print(data_linearb_names_split.head())\n",
    "\n",
    "#apply to the original, non-name dataset as well\n",
    "name_binary=[]\n",
    "for i in range(len(data_linearb_split)):\n",
    "    cur_linearb=data_linearb_split[\"linear_b\"].iloc[i]\n",
    "    corresponding_name=data_linearb_names_split[data_linearb_names_split[\"linear_b\"]==cur_linearb][\"greek\"].iloc[0]\n",
    "    if corresponding_name==\"\":\n",
    "        name_binary.append(0)\n",
    "    else: \n",
    "        name_binary.append(1)\n",
    "print(\"Current non-names dataset:\")\n",
    "data_linearb_split[\"Name\"]=name_binary\n",
    "print(data_linearb_split.head())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transliterate\n",
      "  Downloading transliterate-1.10.2-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m279.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.1.0 in /Users/georgiysekretaryuk/miniforge3/envs/dev_env/lib/python3.10/site-packages (from transliterate) (1.16.0)\n",
      "Installing collected packages: transliterate\n",
      "Successfully installed transliterate-1.10.2\n"
     ]
    }
   ],
   "source": [
    "#TRANSLITERATION OF GREEK\n",
    "\n",
    "!pip install transliterate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_transliterate=[]\n",
    "greek_transliterate_names=[]\n",
    "from transliterate import translit, get_available_language_codes\n",
    "for i in range(len(data_linearb_names_split)):\n",
    "    if data_linearb_names_split[\"greek\"].iloc[i]==\"\":greek_transliterate_names.append(\"\")\n",
    "    else:greek_transliterate_names.append(translit(data_linearb_names_split[\"greek\"].iloc[i], reversed=True))\n",
    "for i in range(len(data_linearb_split)):\n",
    "    if data_linearb_split[\"greek\"].iloc[i]==\"\":greek_transliterate.append(\"\")\n",
    "    else:greek_transliterate.append(translit(data_linearb_split[\"greek\"].iloc[i], reversed=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearb_transliterate=[]\n",
    "linearb_transliterate_names=[]\n",
    "for i in range(len(data_linearb_names_split)):\n",
    "    if data_linearb_names_split[\"linear_b\"].iloc[i]==\"\":linearb_transliterate_names.append(\"\")\n",
    "    else:linearb_transliterate_names.append(transliterate_linb(data_linearb_names_split[\"linear_b\"].iloc[i], linb2syl))\n",
    "for i in range(len(data_linearb_split)):\n",
    "    if data_linearb_split[\"linear_b\"].iloc[i]==\"\":linearb_transliterate.append(\"\")\n",
    "    else:linearb_transliterate.append(transliterate_linb(data_linearb_split[\"linear_b\"].iloc[i], linb2syl))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1           aelipotas               aeriqota\n",
      "1     ğ€€ğ€ğ€´ğ€µ   Î±ÎµÎ¸Î¹ÏƒÏ„Î¿Ï‚     0           aethistos                 aetito\n",
      "2     ğ€€ğ€ğ€´ğ€µ      ÎµÎ¸Î¹Î¶Ï‰     0              ethizo                 aetito\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1              adamao                 adamao\n",
      "4     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚     1              adamas                 adamao\n",
      "  linear_b      greek  Name greek_transliterate linear_b_transliterate\n",
      "0    ğ€€ğ€ğ€ªğ€¦ğ€²  Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚     1           aelipotas               aeriqota\n",
      "1     ğ€€ğ€ğ€´ğ€µ                0                                     aetito\n",
      "2     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Î¿     1              adamao                 adamao\n",
      "3     ğ€€ğ€…ğ€”ğ€ƒ     Î±Î´Î±Î¼Î±Ï‚     1              adamas                 adamao\n",
      "4     ğ€€ğ€…ğ€•ğ€¸  Î±Î´Î±Î¼ÎµfÎµÎ¹Ï‚     1           adamefeis                adamewe\n"
     ]
    }
   ],
   "source": [
    "assert(len(greek_transliterate)==len(data_linearb_split))\n",
    "assert(len(greek_transliterate_names)==len(data_linearb_names_split))\n",
    "data_linearb_split[\"greek_transliterate\"]=greek_transliterate\n",
    "data_linearb_names_split[\"greek_transliterate\"]=greek_transliterate_names\n",
    "data_linearb_split[\"linear_b_transliterate\"]=linearb_transliterate\n",
    "data_linearb_names_split[\"linear_b_transliterate\"]=linearb_transliterate_names\n",
    "print(data_linearb_split.head())\n",
    "print(data_linearb_names_split.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn1-3Fg_Xyws"
   },
   "source": [
    "## EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "Analyze the dataset features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "DlKUCwmpX5zR",
    "outputId": "b2a34baa-18a4-4f21-dd4f-c838c4de7690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- DESCRIBING THE COGNATE DATA: -----\n",
      "\n",
      "       linear_b greek_original greek_cog_1 greek_cog_2 greek_cog_3   \n",
      "count       919            919         919         919         919  \\\n",
      "unique      919            918         918         388          87   \n",
      "top       ğ€€ğ€ğ€ªğ€¦ğ€²        ÎµÏ€Î¹|Î¿Ï€Î¹         ÎµÏ€Î¹                           \n",
      "freq          1              2           2         528         833   \n",
      "\n",
      "       greek_cog_4 greek_cog_5  \n",
      "count          919         919  \n",
      "unique          28           7  \n",
      "top                             \n",
      "freq           892         913  \n",
      "\n",
      "----- INFO: -----\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 919 entries, 0 to 918\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   linear_b        919 non-null    object\n",
      " 1   greek_original  919 non-null    object\n",
      " 2   greek_cog_1     919 non-null    object\n",
      " 3   greek_cog_2     919 non-null    object\n",
      " 4   greek_cog_3     919 non-null    object\n",
      " 5   greek_cog_4     919 non-null    object\n",
      " 6   greek_cog_5     919 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 50.4+ KB\n",
      "None\n",
      "\n",
      "----- CHECKING FOR MISSING VALUES: -----\n",
      "\n",
      "linear_b          0\n",
      "greek_original    0\n",
      "greek_cog_1       0\n",
      "greek_cog_2       0\n",
      "greek_cog_3       0\n",
      "greek_cog_4       0\n",
      "greek_cog_5       0\n",
      "dtype: int64\n",
      "\n",
      "----- CHECKING UNIQUE VALUES: -----\n",
      "\n",
      "linear_b\n",
      "ğ€€ğ€ğ€ªğ€¦ğ€²     1\n",
      "ğ€Ÿğ€©ğ€„ğ€«ğ€™ğ€†    1\n",
      "ğ€Ÿğ€©ğ€ğ€„      1\n",
      "ğ€Ÿğ€©ğ€¦ğ€²      1\n",
      "ğ€Ÿğ€ªğ€•ğ€†      1\n",
      "         ..\n",
      "ğ€„ğŠ        1\n",
      "ğ€…ğ€‚ğ€¦ğ€²      1\n",
      "ğ€…ğ€‚ğ€²ğ€¨ğ€«     1\n",
      "ğ€…ğ€…ğ€©ğ€ğ€†     1\n",
      "ğ‡ğ€œ        1\n",
      "Name: count, Length: 919, dtype: int64\n",
      "greek_original\n",
      "ÎµÏ€Î¹|Î¿Ï€Î¹                  2\n",
      "Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚                1\n",
      "Ï€Î¹Ï€Î·Ï‚                    1\n",
      "Ï€ÏÎµÏƒÏ€Î¿Ï„Î±Ï‚                1\n",
      "Ï€ÎµÏÎ¹Î¼Î·Î´Î·Ï‚                1\n",
      "                        ..\n",
      "Î´Î±Î¹Ï€Î¿Î½Ï„Î±Ï‚                1\n",
      "Î´Î±Î¹Ï„ÏÎ±ÏÎ¿Ï‚                1\n",
      "Î´Î±Î´Î±Î»ÎµÎ¹Î¿Î½|Î´Î±Î´Î±Î»ÎµÎ¹Î¿Î½Î´Îµ    1\n",
      "Î´Î±Î¹Î±ÏÎ¿Ï‚                  1\n",
      "Ï†Ï„ÎµÎ½Î¿Î¹                   1\n",
      "Name: count, Length: 918, dtype: int64\n",
      "greek_cog_1\n",
      "ÎµÏ€Î¹          2\n",
      "Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚    1\n",
      "Ï€Î¹Ï€Î·Ï‚        1\n",
      "Ï€ÏÎµÏƒÏ€Î¿Ï„Î±Ï‚    1\n",
      "Ï€ÎµÏÎ¹Î¼Î·Î´Î·Ï‚    1\n",
      "            ..\n",
      "Î´Î±Î¹Ï€Î¿Î½Ï„Î±Ï‚    1\n",
      "Î´Î±Î¹Ï„ÏÎ±ÏÎ¿Ï‚    1\n",
      "Î´Î±Î´Î±Î»ÎµÎ¹Î¿Î½    1\n",
      "Î´Î±Î¹Î±ÏÎ¿Ï‚      1\n",
      "Ï†Ï„ÎµÎ½Î¿Î¹       1\n",
      "Name: count, Length: 918, dtype: int64\n",
      "greek_cog_2\n",
      "              528\n",
      "Î´Î¹Î´Ï‰            2\n",
      "ÎºÎ¿Ï…ÏÎ·Î¹Î±         2\n",
      "Î¿Ï€Î¹             2\n",
      "Î¿ÏÏ‰             2\n",
      "             ... \n",
      "Î¿Î´ÏÏ…Ï‚           1\n",
      "Î¿Ï…Ï„Îµ            1\n",
      "Î¿Ï…Î´Î¹Î´Î¿Î½Ï„Î¿Î¹      1\n",
      "Î¹Î¸ÎµÎ¹Î±Ï‰Î½         1\n",
      "Î´Ï…fÎ¿            1\n",
      "Name: count, Length: 388, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics and exploration\n",
    "\n",
    "print('\\n----- DESCRIBING THE COGNATE DATA: -----\\n')\n",
    "print(data_linearb.describe())\n",
    "\n",
    "print('\\n----- INFO: -----\\n')\n",
    "print(data_linearb.info())\n",
    "\n",
    "# Check for missing values\n",
    "print('\\n----- CHECKING FOR MISSING VALUES: -----\\n')\n",
    "print(data_linearb.isnull().sum())\n",
    "\n",
    "# Explore unique values and frequency distribution\n",
    "print('\\n----- CHECKING UNIQUE VALUES: -----\\n')\n",
    "print(data_linearb['linear_b'].value_counts())\n",
    "print(data_linearb['greek_original'].value_counts())\n",
    "print(data_linearb['greek_cog_1'].value_counts())\n",
    "print(data_linearb['greek_cog_2'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "EOtKu8kMS03N",
    "outputId": "e35b3c79-63f0-48be-ad06-50ff082cdbef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- DESCRIBING THE NAMES DATA: -----\n",
      "\n",
      "       linear_b greek_original greek_cog_1 greek_cog_2 greek_cog_3   \n",
      "count       919            919         919         919         919  \\\n",
      "unique      919            456         456         131          16   \n",
      "top       ğ€€ğ€ğ€ªğ€¦ğ€²                                                      \n",
      "freq          1            464         464         789         904   \n",
      "\n",
      "       greek_cog_4 greek_cog_5  \n",
      "count          919         919  \n",
      "unique           5           2  \n",
      "top                             \n",
      "freq           915         918  \n",
      "\n",
      "----- INFO: -----\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 919 entries, 0 to 918\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   linear_b        919 non-null    object\n",
      " 1   greek_original  919 non-null    object\n",
      " 2   greek_cog_1     919 non-null    object\n",
      " 3   greek_cog_2     919 non-null    object\n",
      " 4   greek_cog_3     919 non-null    object\n",
      " 5   greek_cog_4     919 non-null    object\n",
      " 6   greek_cog_5     919 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 50.4+ KB\n",
      "None\n",
      "\n",
      "----- CHECKING FOR MISSING VALUES: -----\n",
      "\n",
      "linear_b          0\n",
      "greek_original    0\n",
      "greek_cog_1       0\n",
      "greek_cog_2       0\n",
      "greek_cog_3       0\n",
      "greek_cog_4       0\n",
      "greek_cog_5       0\n",
      "dtype: int64\n",
      "\n",
      "----- CHECKING UNIQUE VALUES: -----\n",
      "\n",
      "linear_b\n",
      "ğ€€ğ€ğ€ªğ€¦ğ€²     1\n",
      "ğ€Ÿğ€©ğ€„ğ€«ğ€™ğ€†    1\n",
      "ğ€Ÿğ€©ğ€ğ€„      1\n",
      "ğ€Ÿğ€©ğ€¦ğ€²      1\n",
      "ğ€Ÿğ€ªğ€•ğ€†      1\n",
      "         ..\n",
      "ğ€„ğŠ        1\n",
      "ğ€…ğ€‚ğ€¦ğ€²      1\n",
      "ğ€…ğ€‚ğ€²ğ€¨ğ€«     1\n",
      "ğ€…ğ€…ğ€©ğ€ğ€†     1\n",
      "ğ‡ğ€œ        1\n",
      "Name: count, Length: 919, dtype: int64\n",
      "greek_original\n",
      "                464\n",
      "Ï€ÎµÏÎ¹Î¸Î¿fÎ¿Ï‚         1\n",
      "Ï€Î¿Î»Î¹fÎ¿Ï‚           1\n",
      "Ï€Î¿ÏÏ†Ï…ÏÎ¹Ï‰Î½         1\n",
      "Ï€Î¿Î´Î±ÏÎ³Î¿Ï‚          1\n",
      "               ... \n",
      "Ï…Î»ÎµÏ…Ï‚             1\n",
      "Ï…Î»Î±Î¼Î½Î¿Ï‚           1\n",
      "Ï…Î»Î±Î¹Î¿Ï‚            1\n",
      "Î¿fÎ¹Ï„Î½Î¿Ï‚           1\n",
      "Ï†Ï…Î»Î¹Î±Ï‚Î±Î³ÏÎµÏ…Ï‚      1\n",
      "Name: count, Length: 456, dtype: int64\n",
      "greek_cog_1\n",
      "                464\n",
      "Ï€ÎµÏÎ¹Î¸Î¿fÎ¿Ï‚         1\n",
      "Ï€Î¿Î»Î¹fÎ¿Ï‚           1\n",
      "Ï€Î¿ÏÏ†Ï…ÏÎ¹Ï‰Î½         1\n",
      "Ï€Î¿Î´Î±ÏÎ³Î¿Ï‚          1\n",
      "               ... \n",
      "Ï…Î»ÎµÏ…Ï‚             1\n",
      "Ï…Î»Î±Î¼Î½Î¿Ï‚           1\n",
      "Ï…Î»Î±Î¹Î¿Ï‚            1\n",
      "Î¿fÎ¹Ï„Î½Î¿Ï‚           1\n",
      "Ï†Ï…Î»Î¹Î±Ï‚Î±Î³ÏÎµÏ…Ï‚      1\n",
      "Name: count, Length: 456, dtype: int64\n",
      "greek_cog_2\n",
      "              789\n",
      "Ï€ÎµÏ€Î¹Î¸Î¼ÎµÎ½Î¿Ï‚      1\n",
      "ÏƒÏ†Î±Î³Î¹Î±Î½Î¹Î¿Î¹      1\n",
      "Ï†Î±ÏƒÎ³Î¹Î±Î½Î±Î¹       1\n",
      "Ï€Î±Î¹Î±fÏ‰Î½         1\n",
      "             ... \n",
      "ÎµÎ»ÎµÏ…Î¸ÎµÏÎµÎ¹       1\n",
      "ÎµÎ»Î¿Ï‚            1\n",
      "ÎµÏÎ±Î¸ÏÎµfÎµÎ¹       1\n",
      "ÎµÎ»Î±Ï„Î¿Î½Î´Îµ        1\n",
      "Î±Ï…Î³ÎµÏ…Ï‚          1\n",
      "Name: count, Length: 131, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('\\n----- DESCRIBING THE NAMES DATA: -----\\n')\n",
    "print(data_linearb_names.describe())\n",
    "\n",
    "print('\\n----- INFO: -----\\n')\n",
    "print(data_linearb_names.info())\n",
    "\n",
    "# Check for missing values\n",
    "print('\\n----- CHECKING FOR MISSING VALUES: -----\\n')\n",
    "print(data_linearb_names.isnull().sum())\n",
    "\n",
    "# Explore unique values and frequency distribution\n",
    "print('\\n----- CHECKING UNIQUE VALUES: -----\\n')\n",
    "print(data_linearb_names['linear_b'].value_counts())\n",
    "print(data_linearb_names['greek_original'].value_counts())\n",
    "print(data_linearb_names['greek_cog_1'].value_counts())\n",
    "print(data_linearb_names['greek_cog_2'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFOJEmRTS03O"
   },
   "source": [
    "## SPLITTING & TOKENIZATION\n",
    "\n",
    "- Breakdown the words into characters\n",
    "- ???\n",
    "- Split the data into test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "ChnA4BoES03O"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/41/hphs227s057g8xhj28fpxshr0000gn/T/ipykernel_35472/316508801.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_linearb_names_train['linear_b_tokens'] = data_linearb_names_train['linear_b'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
      "/var/folders/41/hphs227s057g8xhj28fpxshr0000gn/T/ipykernel_35472/316508801.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_linearb_names_train['greek_tokens'] = data_linearb_names_train['greek'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
      "/var/folders/41/hphs227s057g8xhj28fpxshr0000gn/T/ipykernel_35472/316508801.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_linearb_names_test['linear_b_tokens'] = data_linearb_names_test['linear_b'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n"
     ]
    }
   ],
   "source": [
    "# @title: Splitting & tokenizing the data\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "data_linearb_names_train['linear_b_tokens'] = data_linearb_names_train['linear_b'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "data_linearb_names_train['greek_tokens'] = data_linearb_names_train['greek'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "data_linearb_names_test['linear_b_tokens'] = data_linearb_names_test['linear_b'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "data_linearb['greek_cog_1_tokens'] = data_linearb['greek_cog_1'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "data_linearb['greek_cog_2_tokens'] = data_linearb['greek_cog_2'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True) if x else [])\n",
    "\n",
    "\n",
    "# NEED TO TOKENIZE NAMES AND OTHER DATASETS THAT ARE LOADED HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hcpc27jJX7JU"
   },
   "source": [
    "## MODEL ARCHITECTURE\n",
    "\n",
    "- Identify baseline model\n",
    "- Test other Seq2seq models\n",
    "  - Transformer model - our own?\n",
    "  - Or can we modify BERT/another model and train it too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THE BART MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73e175b77d9841e3970ffaae311903e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245e586da34f44e184e1c0c8fe2c4f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb Cell 30\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m article_A \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInput A\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m article_B \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mExpected Output B\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m MBartForConditionalGeneration\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/mbart-large-50-many-to-many-mmt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m MBart50TokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mfacebook/mbart-large-50-many-to-many-mmt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/georgiysekretaryuk/Library/CloudStorage/OneDrive-Personal/Documents/School/UCBerkeley/Classes/W266_NLP/w266-final-project/lineara_workfile_4.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# translate Linear B to Greek\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/site-packages/transformers/modeling_utils.py:2778\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2763\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2764\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   2765\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m   2766\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcache_dir\u001b[39m\u001b[39m\"\u001b[39m: cache_dir,\n\u001b[1;32m   2767\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mforce_download\u001b[39m\u001b[39m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2776\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m: commit_hash,\n\u001b[1;32m   2777\u001b[0m     }\n\u001b[0;32m-> 2778\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_file_kwargs)\n\u001b[1;32m   2780\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   2781\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   2782\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   2783\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/site-packages/transformers/utils/hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    430\u001b[0m         path_or_repo_id,\n\u001b[1;32m    431\u001b[0m         filename,\n\u001b[1;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1431\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1429\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[0;32m-> 1431\u001b[0m     http_get(\n\u001b[1;32m   1432\u001b[0m         url_to_download,\n\u001b[1;32m   1433\u001b[0m         temp_file,\n\u001b[1;32m   1434\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1435\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m   1436\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1437\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m   1438\u001b[0m     )\n\u001b[1;32m   1440\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1441\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:551\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    541\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(â€¦)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    544\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    545\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[1;32m    550\u001b[0m )\n\u001b[0;32m--> 551\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[1;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    553\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 935\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    937\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    938\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/site-packages/urllib3/response.py:874\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[1;32m    872\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[0;32m--> 874\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[1;32m    876\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/site-packages/urllib3/response.py:809\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    806\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 809\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[1;32m    811\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    818\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    819\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/site-packages/urllib3/response.py:794\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    792\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    793\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/http/client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[1;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[0;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[1;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_env/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "article_A = \"Input A\"\n",
    "article_B = \"Expected Output B\"\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# translate Linear B to Greek\n",
    "tokenizer.src_lang = \"ar_AR\"\n",
    "encoded_ar = tokenizer(article_ar, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_ar, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## T5 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "t5_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = data_linearb_names_split['linear_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_input_texts = [\"translate LinearB to Greek: \" + str(entry) for entry in top_10_articles]\n",
    "t5_inputs = t5_tokenizer(t5_input_texts, return_tensors='tf', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                   num_beams=3,\n",
    "                                   no_repeat_ngram_size=3,\n",
    "                                   min_length=10,\n",
    "                                   max_length=40)\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BART MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any existing file/directory with the same name\n",
    "!rm -rf bart.large.tar.gz\n",
    "\n",
    "# Download the BART model\n",
    "!wget https://dl.fbaipublicfiles.com/fairseq/models/bart.large.tar.gz\n",
    "\n",
    "# Extract the tar file\n",
    "!tar -xzvf bart.large.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = './bart.large.tar.gz'\n",
    "bart = BARTModel.from_pretrained(model_directory, checkpoint_file='model.pt')\n",
    "bart.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = bart.encode('Î±ÎµÎ»Î¹Ï€Î¿Ï„Î±Ï‚')\n",
    "print(\"Encoded tokens:\", tokens.tolist())\n",
    "decoded_text = bart.decode(tokens)\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_features = bart.extract_features(tokens)\n",
    "\n",
    "assert last_layer_features.size() == torch.Size([1, len(tokens), bart.model.encoder.embed_tokens.embedding_dim])\n",
    "\n",
    "all_layers = bart.extract_features(tokens, return_all_hiddens=True)\n",
    "\n",
    "assert len(all_layers) == bart.model.encoder.layers.__len__() + 1  # +1 for the embedding layer\n",
    "assert torch.all(all_layers[-1] == last_layer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jXBtljgS03P"
   },
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUxpeJLjX89Y"
   },
   "outputs": [],
   "source": [
    "# Loading BERT\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "bert_model = BertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYBwnmy1S03P"
   },
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03H5YWQ3S03Q"
   },
   "outputs": [],
   "source": [
    "# Building the COgnate model (sample skeleton)\n",
    "\n",
    "class CognatePredictionModel(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(CognatePredictionModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "\n",
    "        # BERT outputs a 768-d vector\n",
    "        bert_output_size = 768\n",
    "\n",
    "        # Additional fully connected layers\n",
    "        self.fc1 = nn.Linear(bert_output_size * 2, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        # Output layer for binary classification\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, linear_b_tokens, greek_tokens):\n",
    "        # Pass input through BERT, take pooled output\n",
    "        outputs_linear_b = self.bert(linear_b_tokens)[1]\n",
    "        outputs_greek = self.bert(greek_tokens)[1]\n",
    "\n",
    "        # Concatenate the outputs\n",
    "        combined = torch.cat((outputs_linear_b, outputs_greek), 1)\n",
    "\n",
    "        # Pass through additional layers; placeholders\n",
    "        x = self.fc1(combined)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        #print x\n",
    "        # Should be tensor with logits\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHW72bz1S03Q"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pjRsqtmZUfW"
   },
   "source": [
    "## TRAINING\n",
    "\n",
    "- Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Up1TAf_STKbE"
   },
   "outputs": [],
   "source": [
    "unique_greek_tokens = set()\n",
    "\n",
    "for tokens in data_linearb_names_train['greek']:\n",
    "    unique_greek_tokens.update(tokens.split('|'))\n",
    "\n",
    "for tokens in data_linearb_names_test['greek']:\n",
    "    unique_greek_tokens.update(tokens.split('|'))\n",
    "\n",
    "token_to_id = {token: idx for idx, token in enumerate(unique_greek_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLn_2sABZZ7j"
   },
   "outputs": [],
   "source": [
    "class CognateDataset(Dataset):\n",
    "    def __init__(self, linear_b_tokens, greek_tokens, token_to_id, default_id=0):\n",
    "        self.linear_b_tokens = linear_b_tokens\n",
    "        self.greek_tokens = greek_tokens\n",
    "        self.token_to_id = token_to_id\n",
    "        self.default_id = default_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.linear_b_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        linear_b_token_tensor = torch.tensor(self.linear_b_tokens[idx], dtype=torch.long)\n",
    "        greek_token_tensor = torch.tensor(self.greek_tokens[idx], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'linear_b_tokens': linear_b_token_tensor,\n",
    "            'greek_tokens': greek_token_tensor\n",
    "        }\n",
    "\n",
    "train_dataset = CognateDataset(\n",
    "    data_linearb_names_train['linear_b_tokens'].tolist(),\n",
    "    data_linearb_names_train['greek_tokens'].tolist(),\n",
    "    token_to_id,\n",
    "    default_id=0\n",
    ")\n",
    "\n",
    "test_dataset = CognateDataset(\n",
    "    data_linearb_names_test['linear_b_tokens'].tolist(),\n",
    "    # For test data, you might not have labels or might handle them differently\n",
    "    [0] * len(data_linearb_names_test),  # Placeholder if you don't have labels\n",
    "    token_to_id,\n",
    "    default_id=0\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    linear_b_tokens = [item['linear_b_tokens'] for item in batch]\n",
    "    greek_tokens = [item['greek_tokens'] for item in batch]\n",
    "\n",
    "    # Pad sequences\n",
    "    linear_b_tokens_padded = pad_sequence(linear_b_tokens, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    greek_tokens_padded = pad_sequence(greek_tokens, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    return {\n",
    "        'linear_b_tokens': linear_b_tokens_padded,\n",
    "        'greek_tokens': greek_tokens_padded\n",
    "    }\n",
    "\n",
    "data_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHyiuSm8Tpr9"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "model = CognatePredictionModel(bert_model)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        linear_b_tokens = batch['linear_b_tokens']\n",
    "        greek_tokens = batch['greek_tokens']\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(linear_b_tokens, greek_tokens)\n",
    "        outputs = outputs.squeeze()\n",
    "\n",
    "        loss = loss_function(outputs, greek_tokens.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels = (outputs > 0).float()\n",
    "        correct_predictions += (predicted_labels == greek_tokens).sum().item()\n",
    "        total_predictions += greek_tokens.numel()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCwEcUksZCCv"
   },
   "source": [
    "## EVALUATION\n",
    "\n",
    "- The primary goal metric is accuracy as compared to NeuroDecipher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fs1bKF0ZJJp"
   },
   "outputs": [],
   "source": [
    "# Evaluation code\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (test)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
